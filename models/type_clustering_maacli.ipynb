{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e35ec08d",
   "metadata": {},
   "source": [
    "# End-to-End Clustering Analysis with MAACLI Framework\n",
    "\n",
    "This notebook implements a comprehensive clustering pipeline with:\n",
    "- PCA dimensionality reduction\n",
    "- Correlation analysis\n",
    "- KMeans clustering with Optuna hyperparameter tuning\n",
    "- MAACLI framework for interpretability\n",
    "- Cluster quantile profiling\n",
    "\n",
    "**MAACLI Framework Components:**\n",
    "- **M**odel-**A**gnostic: Works with any clustering algorithm\n",
    "- **A**lgorithm-**A**gnostic: Independent of specific ML algorithms\n",
    "- **C**luster **L**abel **I**nterpretation: Uses surrogate models for explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3041641",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10b3496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and numerical libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn components\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import silhouette_score, classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "# XGBoost for surrogate modeling\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except ImportError:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "    print(\"XGBoost not available, will use DecisionTreeClassifier as surrogate model\")\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e816f55",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398d9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: data.csv not found. Using sample data for demonstration.\")\n",
    "    # Create sample data for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    df = pd.DataFrame({\n",
    "        'feature_1': np.random.normal(0, 1, n_samples),\n",
    "        'feature_2': np.random.normal(2, 1.5, n_samples),\n",
    "        'feature_3': np.random.exponential(2, n_samples),\n",
    "        'feature_4': np.random.uniform(-5, 5, n_samples),\n",
    "        'feature_5': np.random.gamma(2, 2, n_samples),\n",
    "        'categorical_feature': np.random.choice(['A', 'B', 'C'], n_samples)\n",
    "    })\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\n=== Dataset Preview ===\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efc1913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset information\n",
    "print(\"\\n=== Dataset Information ===\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\n=== Dataset Statistics ===\")\n",
    "print(df.describe())\n",
    "\n",
    "print(\"\\n=== Missing Values Count ===\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8c1ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically detect numerical columns\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical columns ({len(numerical_columns)}): {numerical_columns}\")\n",
    "print(f\"Categorical columns ({len(categorical_columns)}): {categorical_columns}\")\n",
    "\n",
    "if len(numerical_columns) == 0:\n",
    "    raise ValueError(\"No numerical columns found for clustering analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c9c7b4",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684262b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, numerical_columns):\n",
    "    \"\"\"\n",
    "    Preprocess the dataset for clustering analysis.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        numerical_columns: List of numerical column names\n",
    "    \n",
    "    Returns:\n",
    "        X_scaled: Scaled feature matrix\n",
    "        scaler: Fitted StandardScaler object\n",
    "        df_processed: Processed dataframe\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original data\n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    # Handle missing values (simple imputation with median)\n",
    "    for col in numerical_columns:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            median_val = df_processed[col].median()\n",
    "            df_processed[col].fillna(median_val, inplace=True)\n",
    "            print(f\"Filled {df_processed[col].isnull().sum()} missing values in {col} with median: {median_val:.2f}\")\n",
    "    \n",
    "    # Extract numerical features\n",
    "    X = df_processed[numerical_columns].copy()\n",
    "    \n",
    "    # Scale the data\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(f\"\\nPreprocessing completed!\")\n",
    "    print(f\"Feature matrix shape: {X_scaled.shape}\")\n",
    "    print(f\"Features used: {numerical_columns}\")\n",
    "    \n",
    "    return X_scaled, scaler, df_processed\n",
    "\n",
    "# Apply preprocessing\n",
    "X_scaled, scaler, df_processed = preprocess_data(df, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0334b1c2",
   "metadata": {},
   "source": [
    "## 4. Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f96e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pca(X_scaled, max_components=10):\n",
    "    \"\"\"\n",
    "    Apply PCA for dimensionality reduction.\n",
    "    \n",
    "    Args:\n",
    "        X_scaled: Scaled feature matrix\n",
    "        max_components: Maximum number of components\n",
    "    \n",
    "    Returns:\n",
    "        X_pca: PCA-transformed features\n",
    "        pca: Fitted PCA object\n",
    "    \"\"\"\n",
    "    n_features = X_scaled.shape[1]\n",
    "    n_components = min(max_components, n_features)\n",
    "    \n",
    "    # Apply PCA\n",
    "    pca = PCA(n_components=n_components, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Print explained variance ratios\n",
    "    print(f\"\\n=== PCA Results ===\")\n",
    "    print(f\"Number of components: {n_components}\")\n",
    "    print(f\"Explained variance ratios:\")\n",
    "    \n",
    "    cumulative_variance = 0\n",
    "    for i, ratio in enumerate(pca.explained_variance_ratio_):\n",
    "        cumulative_variance += ratio\n",
    "        print(f\"  PC{i+1}: {ratio:.4f} (cumulative: {cumulative_variance:.4f})\")\n",
    "    \n",
    "    print(f\"\\nTotal explained variance: {cumulative_variance:.4f}\")\n",
    "    \n",
    "    return X_pca, pca\n",
    "\n",
    "# Apply PCA\n",
    "X_pca, pca = apply_pca(X_scaled, max_components=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba765a79",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1731446b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_correlations(df, numerical_columns):\n",
    "    \"\"\"\n",
    "    Compute and display correlation matrix.\n",
    "    \n",
    "    Args:\n",
    "        df: Input dataframe\n",
    "        numerical_columns: List of numerical columns\n",
    "    \"\"\"\n",
    "    # Compute correlation matrix\n",
    "    correlation_matrix = df[numerical_columns].corr()\n",
    "    \n",
    "    print(\"\\n=== Correlation Matrix ===\")\n",
    "    print(correlation_matrix.round(3))\n",
    "    \n",
    "    # Find highly correlated pairs (>0.8 or <-0.8)\n",
    "    high_corr_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_val = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_val) > 0.8:\n",
    "                high_corr_pairs.append((\n",
    "                    correlation_matrix.columns[i], \n",
    "                    correlation_matrix.columns[j], \n",
    "                    corr_val\n",
    "                ))\n",
    "    \n",
    "    if high_corr_pairs:\n",
    "        print(\"\\n=== Highly Correlated Features (|r| > 0.8) ===\")\n",
    "        for feat1, feat2, corr in high_corr_pairs:\n",
    "            print(f\"{feat1} <-> {feat2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"\\nNo highly correlated feature pairs found (|r| > 0.8)\")\n",
    "    \n",
    "    return correlation_matrix\n",
    "\n",
    "# Analyze correlations\n",
    "correlation_matrix = analyze_correlations(df_processed, numerical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633d3fa4",
   "metadata": {},
   "source": [
    "## 6. KMeans Clustering with Optuna Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e98fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_kmeans(X, n_trials=50):\n",
    "    \"\"\"\n",
    "    Optimize KMeans hyperparameters using Optuna.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix for clustering\n",
    "        n_trials: Number of optimization trials\n",
    "    \n",
    "    Returns:\n",
    "        best_params: Best hyperparameters found\n",
    "        best_score: Best silhouette score achieved\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        # Define hyperparameter search space\n",
    "        n_clusters = trial.suggest_int('n_clusters', 2, 12)\n",
    "        init_method = trial.suggest_categorical('init', ['k-means++', 'random'])\n",
    "        n_init = trial.suggest_int('n_init', 10, 50)\n",
    "        \n",
    "        # Fit KMeans with suggested parameters\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=n_clusters,\n",
    "            init=init_method,\n",
    "            n_init=n_init,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        cluster_labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Calculate silhouette score (objective to maximize)\n",
    "        score = silhouette_score(X, cluster_labels)\n",
    "        \n",
    "        return score\n",
    "    \n",
    "    # Create and run optimization study\n",
    "    study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "    study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    best_score = study.best_value\n",
    "    \n",
    "    print(f\"\\n=== Optuna Optimization Results ===\")\n",
    "    print(f\"Best silhouette score: {best_score:.4f}\")\n",
    "    print(f\"Best parameters:\")\n",
    "    for param, value in best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    return best_params, best_score\n",
    "\n",
    "# Optimize KMeans hyperparameters\n",
    "print(\"Starting hyperparameter optimization...\")\n",
    "best_params, best_score = optimize_kmeans(X_pca, n_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064762b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the best KMeans model\n",
    "best_kmeans = KMeans(\n",
    "    n_clusters=best_params['n_clusters'],\n",
    "    init=best_params['init'],\n",
    "    n_init=best_params['n_init'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "cluster_labels = best_kmeans.fit_predict(X_pca)\n",
    "\n",
    "# Add cluster labels to the dataframe\n",
    "df_clustered = df_processed.copy()\n",
    "df_clustered['cluster_label'] = cluster_labels\n",
    "\n",
    "print(f\"\\n=== Clustering Results ===\")\n",
    "print(f\"Number of clusters: {best_params['n_clusters']}\")\n",
    "print(f\"Silhouette score: {best_score:.4f}\")\n",
    "print(f\"\\nCluster sizes:\")\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"  Cluster {cluster}: {count} samples ({count/len(cluster_labels)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5bd180",
   "metadata": {},
   "source": [
    "## 7. MAACLI Framework Implementation\n",
    "\n",
    "### Model-Agnostic Algorithm-Agnostic Cluster Label Interpretation\n",
    "\n",
    "The MAACLI framework provides interpretability through:\n",
    "- **Model-Agnostic**: Works with any clustering algorithm\n",
    "- **Algorithm-Agnostic**: Uses surrogate models for interpretation\n",
    "- **Local + Global Interpretability**: Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7b4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_surrogate_model(X, y, use_xgboost=True):\n",
    "    \"\"\"\n",
    "    Train a surrogate classifier to interpret cluster assignments.\n",
    "    \n",
    "    This implements the core of the MAACLI framework:\n",
    "    - Uses original features to predict cluster labels\n",
    "    - Provides feature importance for cluster interpretation\n",
    "    \n",
    "    Args:\n",
    "        X: Original feature matrix (not PCA-transformed)\n",
    "        y: Cluster labels\n",
    "        use_xgboost: Whether to use XGBoost (if available)\n",
    "    \n",
    "    Returns:\n",
    "        surrogate_model: Trained surrogate classifier\n",
    "        feature_importance: Feature importance scores\n",
    "        X_train, X_test, y_train, y_test: Train/test splits\n",
    "    \"\"\"\n",
    "    # Split data for surrogate model training\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Choose surrogate model\n",
    "    if use_xgboost and XGBOOST_AVAILABLE:\n",
    "        print(\"\\n=== Training XGBoost Surrogate Model ===\")\n",
    "        surrogate_model = xgb.XGBClassifier(\n",
    "            random_state=42,\n",
    "            eval_metric='mlogloss'\n",
    "        )\n",
    "        model_type = \"XGBoost\"\n",
    "    else:\n",
    "        print(\"\\n=== Training Decision Tree Surrogate Model ===\")\n",
    "        surrogate_model = DecisionTreeClassifier(\n",
    "            random_state=42,\n",
    "            max_depth=10,\n",
    "            min_samples_split=20,\n",
    "            min_samples_leaf=10\n",
    "        )\n",
    "        model_type = \"Decision Tree\"\n",
    "    \n",
    "    # Train the surrogate model\n",
    "    surrogate_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = surrogate_model.predict(X_test)\n",
    "    \n",
    "    # Evaluate surrogate model performance\n",
    "    accuracy = (y_pred == y_test).mean()\n",
    "    print(f\"\\n{model_type} Surrogate Model Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Get feature importance\n",
    "    if hasattr(surrogate_model, 'feature_importances_'):\n",
    "        feature_importance = surrogate_model.feature_importances_\n",
    "    else:\n",
    "        feature_importance = np.zeros(X.shape[1])  # Fallback\n",
    "    \n",
    "    return surrogate_model, feature_importance, (X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Train surrogate model using original scaled features\n",
    "surrogate_model, feature_importance, split_data = train_surrogate_model(\n",
    "    X_scaled, cluster_labels, use_xgboost=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edec69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAACLI Interpretation: Feature Importance Analysis\n",
    "print(\"\\n=== MAACLI Framework: Feature Importance Analysis ===\")\n",
    "print(\"This provides Global Interpretability for cluster assignments\\n\")\n",
    "\n",
    "# Create feature importance dataframe\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': numerical_columns,\n",
    "    'importance': feature_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance Ranking:\")\n",
    "for idx, row in importance_df.iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "# Identify top contributing features\n",
    "top_features = importance_df.head(3)['feature'].tolist()\n",
    "print(f\"\\nTop 3 most important features for cluster separation:\")\n",
    "print(f\"{top_features}\")\n",
    "\n",
    "print(\"\\n=== MAACLI Interpretation Summary ===\")\n",
    "print(\"✓ Model-Agnostic: Works with KMeans clustering\")\n",
    "print(\"✓ Algorithm-Agnostic: Uses surrogate classifier for interpretation\")\n",
    "print(\"✓ Global Interpretability: Feature importance across all clusters\")\n",
    "print(\"✓ Local Interpretability: Can predict individual cluster assignments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1772aec6",
   "metadata": {},
   "source": [
    "## 8. Cluster Quantile Profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb3e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_quantiles(df_clustered, numerical_columns, quantiles=[0.05, 0.25, 0.50, 0.75, 0.95]):\n",
    "    \"\"\"\n",
    "    Compute quantile profiles for each cluster.\n",
    "    \n",
    "    Args:\n",
    "        df_clustered: Dataframe with cluster labels\n",
    "        numerical_columns: List of numerical columns\n",
    "        quantiles: List of quantiles to compute\n",
    "    \n",
    "    Returns:\n",
    "        quantile_profiles: Dataframe with quantile profiles\n",
    "    \"\"\"\n",
    "    quantile_data = []\n",
    "    \n",
    "    for cluster in sorted(df_clustered['cluster_label'].unique()):\n",
    "        cluster_data = df_clustered[df_clustered['cluster_label'] == cluster]\n",
    "        \n",
    "        for feature in numerical_columns:\n",
    "            feature_values = cluster_data[feature]\n",
    "            \n",
    "            for q in quantiles:\n",
    "                quantile_value = feature_values.quantile(q)\n",
    "                quantile_data.append({\n",
    "                    'cluster': cluster,\n",
    "                    'feature': feature,\n",
    "                    'quantile': f'Q{int(q*100):02d}',\n",
    "                    'value': quantile_value\n",
    "                })\n",
    "    \n",
    "    quantile_profiles = pd.DataFrame(quantile_data)\n",
    "    return quantile_profiles\n",
    "\n",
    "# Compute cluster quantile profiles\n",
    "quantile_profiles = compute_cluster_quantiles(df_clustered, numerical_columns)\n",
    "\n",
    "print(\"=== Cluster Quantile Profiles ===\")\n",
    "print(\"\\nThis table shows the distribution characteristics of each feature within each cluster\")\n",
    "print(\"Quantiles: Q05 (5%), Q25 (25%), Q50 (50%/median), Q75 (75%), Q95 (95%)\\n\")\n",
    "\n",
    "# Display quantile table in a readable format\n",
    "for cluster in sorted(df_clustered['cluster_label'].unique()):\n",
    "    print(f\"\\n--- Cluster {cluster} ---\")\n",
    "    cluster_profile = quantile_profiles[quantile_profiles['cluster'] == cluster]\n",
    "    \n",
    "    # Pivot to get features as rows and quantiles as columns\n",
    "    pivot_table = cluster_profile.pivot(index='feature', columns='quantile', values='value')\n",
    "    print(pivot_table.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d096aec8",
   "metadata": {},
   "source": [
    "## 9. Results Summary and Data Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df6b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print comprehensive summary\n",
    "print(\"=== COMPLETE CLUSTERING ANALYSIS SUMMARY ===\")\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"  • Original shape: {df.shape}\")\n",
    "print(f\"  • Features used: {len(numerical_columns)} numerical features\")\n",
    "print(f\"  • Features: {numerical_columns}\")\n",
    "\n",
    "print(f\"\\nPCA Results:\")\n",
    "print(f\"  • Components: {X_pca.shape[1]}\")\n",
    "print(f\"  • Total explained variance: {sum(pca.explained_variance_ratio_):.3f}\")\n",
    "\n",
    "print(f\"\\nOptimal Clustering Configuration:\")\n",
    "print(f\"  • Number of clusters: {best_params['n_clusters']}\")\n",
    "print(f\"  • Initialization: {best_params['init']}\")\n",
    "print(f\"  • N_init: {best_params['n_init']}\")\n",
    "print(f\"  • Silhouette score: {best_score:.4f}\")\n",
    "\n",
    "print(f\"\\nCluster Distribution:\")\n",
    "for cluster, count in cluster_counts.items():\n",
    "    print(f\"  • Cluster {cluster}: {count} samples ({count/len(cluster_labels)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nMAACLI Interpretability:\")\n",
    "print(f\"  • Surrogate model accuracy: {(split_data[3] == surrogate_model.predict(split_data[1])).mean():.3f}\")\n",
    "print(f\"  • Top 3 important features: {top_features}\")\n",
    "\n",
    "# Save results\n",
    "try:\n",
    "    # Save enriched dataset with cluster labels\n",
    "    df_clustered.to_csv('clustered_data.csv', index=False)\n",
    "    print(f\"\\n✓ Clustered dataset saved as 'clustered_data.csv'\")\n",
    "    \n",
    "    # Save quantile summary\n",
    "    quantile_summary = quantile_profiles.pivot_table(\n",
    "        index=['cluster', 'feature'], \n",
    "        columns='quantile', \n",
    "        values='value'\n",
    "    )\n",
    "    quantile_summary.to_csv('cluster_quantile_profiles.csv')\n",
    "    print(f\"✓ Quantile profiles saved as 'cluster_quantile_profiles.csv'\")\n",
    "    \n",
    "    # Save feature importance\n",
    "    importance_df.to_csv('feature_importance_maacli.csv', index=False)\n",
    "    print(f\"✓ Feature importance saved as 'feature_importance_maacli.csv'\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not save files - {e}\")\n",
    "\n",
    "print(f\"\\n=== Analysis Complete ===\")\n",
    "print(f\"This clustering analysis successfully implemented:\")\n",
    "print(f\"  ✓ End-to-end preprocessing pipeline\")\n",
    "print(f\"  ✓ PCA dimensionality reduction\")\n",
    "print(f\"  ✓ Correlation analysis\")\n",
    "print(f\"  ✓ Optuna hyperparameter optimization\")\n",
    "print(f\"  ✓ MAACLI interpretability framework\")\n",
    "print(f\"  ✓ Comprehensive cluster profiling\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
