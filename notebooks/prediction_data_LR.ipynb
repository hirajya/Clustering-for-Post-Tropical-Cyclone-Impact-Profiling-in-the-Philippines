{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9013e8a8",
   "metadata": {},
   "source": [
    "# Linear Regression Training Pipeline\n",
    "\n",
    "This notebook implements an end-to-end Linear Regression training pipeline with:\n",
    "- Data loading and exploration\n",
    "- Preprocessing and scaling\n",
    "- Train-test split\n",
    "- Model training\n",
    "- Hyperparameter tuning with Optuna\n",
    "- Model evaluation and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48b1b35",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131fe740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae04b5d",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264747f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(\"data.csv\")\n",
    "    print(f\"Dataset loaded successfully! Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: data.csv file not found. Please ensure the file exists in the current directory.\")\n",
    "    print(\"For demonstration purposes, creating a synthetic dataset...\")\n",
    "    \n",
    "    # Create a synthetic dataset for demonstration\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1000\n",
    "    df = pd.DataFrame({\n",
    "        'feature_1': np.random.normal(50, 15, n_samples),\n",
    "        'feature_2': np.random.normal(100, 25, n_samples),\n",
    "        'feature_3': np.random.uniform(0, 100, n_samples),\n",
    "        'feature_4': np.random.exponential(2, n_samples),\n",
    "        'category': np.random.choice(['A', 'B', 'C'], n_samples)\n",
    "    })\n",
    "    \n",
    "    # Create target variable with some relationship to features\n",
    "    df['target'] = (df['feature_1'] * 0.5 + \n",
    "                   df['feature_2'] * 0.3 + \n",
    "                   df['feature_3'] * 0.2 + \n",
    "                   np.random.normal(0, 10, n_samples))\n",
    "    \n",
    "    print(f\"Synthetic dataset created! Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e0fd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Head:\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Dataset Description:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Missing Values Count:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7f7850",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e40ad728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy for preprocessing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Handle missing values (simple approach)\n",
    "print(\"Handling missing values...\")\n",
    "if df_processed.isnull().sum().sum() > 0:\n",
    "    # For numeric columns, fill with median\n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns\n",
    "    for col in numeric_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col].fillna(df_processed[col].median(), inplace=True)\n",
    "    \n",
    "    # For categorical columns, fill with mode\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        if df_processed[col].isnull().sum() > 0:\n",
    "            df_processed[col].fillna(df_processed[col].mode()[0], inplace=True)\n",
    "    \n",
    "    print(\"Missing values handled.\")\n",
    "else:\n",
    "    print(\"No missing values found.\")\n",
    "\n",
    "# Select numeric columns for features (excluding target)\n",
    "numeric_columns = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"\\nNumeric columns found: {numeric_columns}\")\n",
    "\n",
    "# Assume the last numeric column is the target (or specify manually)\n",
    "if 'target' in numeric_columns:\n",
    "    target_column = 'target'\n",
    "elif 'Target' in numeric_columns:\n",
    "    target_column = 'Target'\n",
    "else:\n",
    "    # Use the last numeric column as target\n",
    "    target_column = numeric_columns[-1]\n",
    "    \n",
    "print(f\"Target column: {target_column}\")\n",
    "\n",
    "# Create feature matrix X and target vector y\n",
    "feature_columns = [col for col in numeric_columns if col != target_column]\n",
    "print(f\"Feature columns: {feature_columns}\")\n",
    "\n",
    "X = df_processed[feature_columns]\n",
    "y = df_processed[target_column]\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce07cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Handle categorical variables if present\n",
    "categorical_cols = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "if categorical_cols:\n",
    "    print(f\"Categorical columns found: {categorical_cols}\")\n",
    "    print(\"Encoding categorical variables using one-hot encoding...\")\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    categorical_data = pd.get_dummies(df_processed[categorical_cols], drop_first=True)\n",
    "    \n",
    "    # Combine with numeric features\n",
    "    X = pd.concat([X, categorical_data], axis=1)\n",
    "    \n",
    "    print(f\"Updated feature matrix shape: {X.shape}\")\n",
    "    print(f\"New feature columns: {list(X.columns)}\")\n",
    "else:\n",
    "    print(\"No categorical columns found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c2c8ca",
   "metadata": {},
   "source": [
    "## 4. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f402264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42,\n",
    "    stratify=None  # For regression, we don't stratify\n",
    ")\n",
    "\n",
    "print(f\"Training set shape: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"Testing set shape: X_test {X_test.shape}, y_test {y_test.shape}\")\n",
    "print(f\"Train-test split ratio: {len(X_train)}/{len(X_test)} = {len(X_train)/len(X_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5469bfc",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171533a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data only\n",
    "print(\"Fitting scaler on training data...\")\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "print(\"Scaling training and testing data...\")\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(f\"Scaled training data shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing data shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# Show scaling statistics\n",
    "print(\"\\nScaling statistics (training data):\")\n",
    "print(f\"Original data mean: {X_train.mean().mean():.4f}\")\n",
    "print(f\"Original data std: {X_train.std().mean():.4f}\")\n",
    "print(f\"Scaled data mean: {X_train_scaled.mean().mean():.4f}\")\n",
    "print(f\"Scaled data std: {X_train_scaled.std().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa0cf1",
   "metadata": {},
   "source": [
    "## 6. Basic Linear Regression Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d068000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train basic Linear Regression model\n",
    "print(\"Training basic Linear Regression model...\")\n",
    "lr_basic = LinearRegression()\n",
    "lr_basic.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Display model parameters\n",
    "print(f\"\\nModel Intercept: {lr_basic.intercept_:.4f}\")\n",
    "print(f\"Number of coefficients: {len(lr_basic.coef_)}\")\n",
    "\n",
    "print(\"\\nModel Coefficients:\")\n",
    "coefficients_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Coefficient': lr_basic.coef_\n",
    "})\n",
    "coefficients_df['Abs_Coefficient'] = np.abs(coefficients_df['Coefficient'])\n",
    "coefficients_df = coefficients_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "print(coefficients_df)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_basic = lr_basic.predict(X_train_scaled)\n",
    "y_test_pred_basic = lr_basic.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nBasic model predictions completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67f8bd9",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28100c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the objective function for Optuna optimization\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Objective function for Optuna hyperparameter optimization.\n",
    "    Tests different regression models and their parameters.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Suggest model type\n",
    "    model_type = trial.suggest_categorical('model_type', ['linear', 'ridge', 'lasso'])\n",
    "    \n",
    "    if model_type == 'linear':\n",
    "        # Linear Regression parameters\n",
    "        fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n",
    "        positive = trial.suggest_categorical('positive', [True, False])\n",
    "        \n",
    "        model = LinearRegression(\n",
    "            fit_intercept=fit_intercept,\n",
    "            positive=positive\n",
    "        )\n",
    "    \n",
    "    elif model_type == 'ridge':\n",
    "        # Ridge Regression parameters\n",
    "        alpha = trial.suggest_float('alpha', 0.01, 100.0, log=True)\n",
    "        fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n",
    "        \n",
    "        model = Ridge(\n",
    "            alpha=alpha,\n",
    "            fit_intercept=fit_intercept,\n",
    "            random_state=42\n",
    "        )\n",
    "    \n",
    "    else:  # lasso\n",
    "        # Lasso Regression parameters\n",
    "        alpha = trial.suggest_float('alpha', 0.01, 10.0, log=True)\n",
    "        fit_intercept = trial.suggest_categorical('fit_intercept', [True, False])\n",
    "        \n",
    "        model = Lasso(\n",
    "            alpha=alpha,\n",
    "            fit_intercept=fit_intercept,\n",
    "            random_state=42,\n",
    "            max_iter=2000\n",
    "        )\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    # Calculate RMSE (objective to minimize)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    \n",
    "    return rmse\n",
    "\n",
    "# Create and run the optimization study\n",
    "print(\"Starting hyperparameter optimization with Optuna...\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "study = optuna.create_study(direction='minimize', study_name='linear_regression_optimization')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "best_value = study.best_value\n",
    "\n",
    "print(f\"\\nOptimization completed!\")\n",
    "print(f\"Best RMSE: {best_value:.4f}\")\n",
    "print(f\"Best parameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2f4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model with best parameters\n",
    "print(\"Training final model with best parameters...\")\n",
    "\n",
    "if best_params['model_type'] == 'linear':\n",
    "    final_model = LinearRegression(\n",
    "        fit_intercept=best_params['fit_intercept'],\n",
    "        positive=best_params['positive']\n",
    "    )\n",
    "elif best_params['model_type'] == 'ridge':\n",
    "    final_model = Ridge(\n",
    "        alpha=best_params['alpha'],\n",
    "        fit_intercept=best_params['fit_intercept'],\n",
    "        random_state=42\n",
    "    )\n",
    "else:  # lasso\n",
    "    final_model = Lasso(\n",
    "        alpha=best_params['alpha'],\n",
    "        fit_intercept=best_params['fit_intercept'],\n",
    "        random_state=42,\n",
    "        max_iter=2000\n",
    "    )\n",
    "\n",
    "# Train the final model\n",
    "final_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred_final = final_model.predict(X_train_scaled)\n",
    "y_test_pred_final = final_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Final model training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf15deb",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6bf29af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, model_name=\"Model\"):\n",
    "    \"\"\"Calculate and return evaluation metrics\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'MAE': mae,\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'R²': r2\n",
    "    }\n",
    "\n",
    "# Calculate metrics for basic model\n",
    "basic_train_metrics = calculate_metrics(y_train, y_train_pred_basic, \"Basic LR (Train)\")\n",
    "basic_test_metrics = calculate_metrics(y_test, y_test_pred_basic, \"Basic LR (Test)\")\n",
    "\n",
    "# Calculate metrics for optimized model\n",
    "final_train_metrics = calculate_metrics(y_train, y_train_pred_final, \"Optimized (Train)\")\n",
    "final_test_metrics = calculate_metrics(y_test, y_test_pred_final, \"Optimized (Test)\")\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame([\n",
    "    basic_train_metrics,\n",
    "    basic_test_metrics,\n",
    "    final_train_metrics,\n",
    "    final_test_metrics\n",
    "])\n",
    "\n",
    "print(\"Model Evaluation Results:\")\n",
    "print(\"=\"*60)\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c21ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed results summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETAILED EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nDataset Information:\")\n",
    "print(f\"- Total samples: {len(df)}\")\n",
    "print(f\"- Features: {len(feature_columns)}\")\n",
    "print(f\"- Training samples: {len(X_train)}\")\n",
    "print(f\"- Testing samples: {len(X_test)}\")\n",
    "\n",
    "print(f\"\\nBest Model Configuration:\")\n",
    "print(f\"- Model type: {best_params['model_type'].upper()}\")\n",
    "for param, value in best_params.items():\n",
    "    if param != 'model_type':\n",
    "        print(f\"- {param}: {value}\")\n",
    "\n",
    "print(f\"\\nFinal Model Performance (Test Set):\")\n",
    "print(f\"- Mean Absolute Error (MAE): {final_test_metrics['MAE']:.4f}\")\n",
    "print(f\"- Mean Squared Error (MSE): {final_test_metrics['MSE']:.4f}\")\n",
    "print(f\"- Root Mean Squared Error (RMSE): {final_test_metrics['RMSE']:.4f}\")\n",
    "print(f\"- R² Score: {final_test_metrics['R²']:.4f}\")\n",
    "\n",
    "# Model performance interpretation\n",
    "r2_test = final_test_metrics['R²']\n",
    "if r2_test > 0.9:\n",
    "    performance = \"Excellent\"\n",
    "elif r2_test > 0.8:\n",
    "    performance = \"Good\"\n",
    "elif r2_test > 0.6:\n",
    "    performance = \"Moderate\"\n",
    "else:\n",
    "    performance = \"Poor\"\n",
    "\n",
    "print(f\"\\nModel Performance Assessment: {performance}\")\n",
    "print(f\"- The model explains {r2_test*100:.2f}% of the variance in the target variable\")\n",
    "\n",
    "# Overfitting check\n",
    "train_r2 = final_train_metrics['R²']\n",
    "r2_diff = train_r2 - r2_test\n",
    "if r2_diff > 0.1:\n",
    "    print(f\"- Warning: Potential overfitting detected (Train R²: {train_r2:.4f}, Test R²: {r2_test:.4f})\")\n",
    "else:\n",
    "    print(f\"- Good generalization (Train R²: {train_r2:.4f}, Test R²: {r2_test:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1864d0e0",
   "metadata": {},
   "source": [
    "## 10. Model Output and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1886819a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictions DataFrame for analysis\n",
    "predictions_df = pd.DataFrame({\n",
    "    'Actual': y_test.values,\n",
    "    'Predicted': y_test_pred_final,\n",
    "    'Residual': y_test.values - y_test_pred_final\n",
    "})\n",
    "\n",
    "predictions_df['Abs_Residual'] = np.abs(predictions_df['Residual'])\n",
    "predictions_df = predictions_df.sort_values('Abs_Residual', ascending=False)\n",
    "\n",
    "print(\"Sample Predictions (Top 10 largest residuals):\")\n",
    "print(predictions_df.head(10).round(4))\n",
    "\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"- Mean Residual: {predictions_df['Residual'].mean():.4f}\")\n",
    "print(f\"- Std Residual: {predictions_df['Residual'].std():.4f}\")\n",
    "print(f\"- Max Absolute Residual: {predictions_df['Abs_Residual'].max():.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LINEAR REGRESSION PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Data loaded and preprocessed\")\n",
    "print(f\"✓ Features scaled using StandardScaler\")\n",
    "print(f\"✓ Train-test split completed (80-20)\")\n",
    "print(f\"✓ Basic Linear Regression model trained\")\n",
    "print(f\"✓ Hyperparameter optimization with Optuna completed\")\n",
    "print(f\"✓ Final optimized model evaluated\")\n",
    "print(f\"✓ Model performance: R² = {final_test_metrics['R²']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab16420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Save model and predictions JOBLIB \n",
    "import joblib\n",
    "\n",
    "# export model using joblib"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
