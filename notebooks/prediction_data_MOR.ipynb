{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9e25774",
   "metadata": {},
   "source": [
    "# Multi-Output Regression for Weather-Based Casualty Prediction\n",
    "\n",
    "This notebook implements a complete multi-output regression pipeline that predicts multiple casualty counts from weather data using XGBoost.\n",
    "\n",
    "## Pipeline Components:\n",
    "- Data loading and exploration\n",
    "- Weather feature preprocessing and engineering\n",
    "- Feature scaling\n",
    "- Train-test split\n",
    "- Multi-output XGBoost model training\n",
    "- Comprehensive evaluation metrics\n",
    "- Results visualization and summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30b3a0c",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219d24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# XGBoost for regression\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(f\"XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc82f02",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac687cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "try:\n",
    "    df = pd.read_csv(\"weather_casualty.csv\")\n",
    "    print(f\"Dataset loaded successfully! Shape: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: weather_casualty.csv file not found. Please ensure the file exists in the current directory.\")\n",
    "    print(\"For demonstration purposes, creating a synthetic dataset...\")\n",
    "    \n",
    "    # Create a synthetic weather-casualty dataset\n",
    "    np.random.seed(42)\n",
    "    n_samples = 1500\n",
    "    \n",
    "    # Weather features\n",
    "    df = pd.DataFrame({\n",
    "        # Temperature features\n",
    "        'max_temperature': np.random.normal(32, 8, n_samples),\n",
    "        'min_temperature': np.random.normal(25, 6, n_samples),\n",
    "        'avg_temperature': np.random.normal(28, 7, n_samples),\n",
    "        \n",
    "        # Humidity features\n",
    "        'max_humidity': np.random.uniform(60, 100, n_samples),\n",
    "        'min_humidity': np.random.uniform(40, 80, n_samples),\n",
    "        'avg_humidity': np.random.uniform(50, 90, n_samples),\n",
    "        \n",
    "        # Wind features\n",
    "        'max_wind_speed': np.random.exponential(15, n_samples),\n",
    "        'avg_wind_speed': np.random.exponential(10, n_samples),\n",
    "        'wind_direction': np.random.uniform(0, 360, n_samples),\n",
    "        \n",
    "        # Precipitation features\n",
    "        'total_rainfall': np.random.exponential(20, n_samples),\n",
    "        'max_rainfall_1hr': np.random.exponential(5, n_samples),\n",
    "        'rainfall_duration': np.random.uniform(0, 24, n_samples),\n",
    "        \n",
    "        # Pressure features\n",
    "        'sea_level_pressure': np.random.normal(1013, 15, n_samples),\n",
    "        'pressure_change': np.random.normal(0, 5, n_samples),\n",
    "        \n",
    "        # Additional weather indicators\n",
    "        'visibility': np.random.uniform(1, 10, n_samples),\n",
    "        'cloud_cover': np.random.uniform(0, 100, n_samples)\n",
    "    })\n",
    "    \n",
    "    # Create casualty targets with realistic relationships to weather\n",
    "    # More severe weather conditions lead to higher casualties\n",
    "    severity_factor = (\n",
    "        (df['max_wind_speed'] / 50) + \n",
    "        (df['total_rainfall'] / 100) + \n",
    "        (np.abs(df['pressure_change']) / 20) +\n",
    "        ((100 - df['visibility']) / 100)\n",
    "    )\n",
    "    \n",
    "    df['deaths'] = np.maximum(0, np.round(\n",
    "        severity_factor * np.random.poisson(2, n_samples) + \n",
    "        np.random.normal(0, 1, n_samples)\n",
    "    ).astype(int))\n",
    "    \n",
    "    df['injured'] = np.maximum(0, np.round(\n",
    "        severity_factor * np.random.poisson(8, n_samples) + \n",
    "        np.random.normal(0, 3, n_samples)\n",
    "    ).astype(int))\n",
    "    \n",
    "    df['missing'] = np.maximum(0, np.round(\n",
    "        severity_factor * np.random.poisson(3, n_samples) + \n",
    "        np.random.normal(0, 2, n_samples)\n",
    "    ).astype(int))\n",
    "    \n",
    "    df['displaced'] = np.maximum(0, np.round(\n",
    "        severity_factor * np.random.poisson(50, n_samples) + \n",
    "        np.random.normal(0, 20, n_samples)\n",
    "    ).astype(int))\n",
    "    \n",
    "    print(f\"Synthetic dataset created! Shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00ed999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"Dataset Head:\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Dataset Info:\")\n",
    "print(df.info())\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Dataset Description:\")\n",
    "print(df.describe())\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "print(\"Missing Values Count:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"No missing values found.\")\n",
    "else:\n",
    "    print(f\"Total missing values: {missing_values.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a336f6a",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91c6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_feature_columns(df):\n",
    "    \"\"\"Identify weather feature columns and casualty target columns\"\"\"\n",
    "    \n",
    "    # Weather-related features (excluding casualty columns)\n",
    "    casualty_keywords = ['death', 'injur', 'missing', 'displace', 'casualt', 'fatali']\n",
    "    \n",
    "    weather_features = []\n",
    "    casualty_targets = []\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if any(keyword in col_lower for keyword in casualty_keywords):\n",
    "            casualty_targets.append(col)\n",
    "        else:\n",
    "            # Assume all other numeric columns are weather features\n",
    "            if df[col].dtype in ['int64', 'float64']:\n",
    "                weather_features.append(col)\n",
    "    \n",
    "    return weather_features, casualty_targets\n",
    "\n",
    "def handle_missing_values(df, weather_features):\n",
    "    \"\"\"Handle missing values in weather features\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    for col in weather_features:\n",
    "        if df_clean[col].isnull().sum() > 0:\n",
    "            # Fill missing values with median for weather data\n",
    "            median_val = df_clean[col].median()\n",
    "            df_clean[col].fillna(median_val, inplace=True)\n",
    "            print(f\"Filled {df[col].isnull().sum()} missing values in {col} with median: {median_val:.2f}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "def engineer_weather_features(df, weather_features):\n",
    "    \"\"\"Create engineered weather features\"\"\"\n",
    "    df_engineered = df.copy()\n",
    "    new_features = []\n",
    "    \n",
    "    # Temperature-based features\n",
    "    if 'max_temperature' in weather_features and 'min_temperature' in weather_features:\n",
    "        df_engineered['temperature_range'] = df_engineered['max_temperature'] - df_engineered['min_temperature']\n",
    "        new_features.append('temperature_range')\n",
    "    \n",
    "    if 'avg_temperature' in weather_features:\n",
    "        df_engineered['temp_extreme_cold'] = (df_engineered['avg_temperature'] < 20).astype(int)\n",
    "        df_engineered['temp_extreme_hot'] = (df_engineered['avg_temperature'] > 35).astype(int)\n",
    "        new_features.extend(['temp_extreme_cold', 'temp_extreme_hot'])\n",
    "    \n",
    "    # Humidity-based features\n",
    "    if 'max_humidity' in weather_features and 'min_humidity' in weather_features:\n",
    "        df_engineered['humidity_range'] = df_engineered['max_humidity'] - df_engineered['min_humidity']\n",
    "        new_features.append('humidity_range')\n",
    "    \n",
    "    if 'avg_humidity' in weather_features:\n",
    "        df_engineered['high_humidity'] = (df_engineered['avg_humidity'] > 80).astype(int)\n",
    "        new_features.append('high_humidity')\n",
    "    \n",
    "    # Wind-based features\n",
    "    if 'max_wind_speed' in weather_features:\n",
    "        df_engineered['wind_category'] = pd.cut(df_engineered['max_wind_speed'], \n",
    "                                               bins=[0, 10, 25, 50, np.inf], \n",
    "                                               labels=['Light', 'Moderate', 'Strong', 'Severe']).astype('category').cat.codes\n",
    "        df_engineered['extreme_wind'] = (df_engineered['max_wind_speed'] > 40).astype(int)\n",
    "        new_features.extend(['wind_category', 'extreme_wind'])\n",
    "    \n",
    "    # Rainfall-based features\n",
    "    if 'total_rainfall' in weather_features:\n",
    "        df_engineered['rainfall_category'] = pd.cut(df_engineered['total_rainfall'], \n",
    "                                                  bins=[0, 5, 20, 50, np.inf], \n",
    "                                                  labels=['Light', 'Moderate', 'Heavy', 'Extreme']).astype('category').cat.codes\n",
    "        df_engineered['heavy_rain'] = (df_engineered['total_rainfall'] > 30).astype(int)\n",
    "        df_engineered['no_rain'] = (df_engineered['total_rainfall'] == 0).astype(int)\n",
    "        new_features.extend(['rainfall_category', 'heavy_rain', 'no_rain'])\n",
    "    \n",
    "    # Pressure-based features\n",
    "    if 'pressure_change' in weather_features:\n",
    "        df_engineered['rapid_pressure_drop'] = (df_engineered['pressure_change'] < -5).astype(int)\n",
    "        df_engineered['rapid_pressure_rise'] = (df_engineered['pressure_change'] > 5).astype(int)\n",
    "        new_features.extend(['rapid_pressure_drop', 'rapid_pressure_rise'])\n",
    "    \n",
    "    # Visibility features\n",
    "    if 'visibility' in weather_features:\n",
    "        df_engineered['poor_visibility'] = (df_engineered['visibility'] < 3).astype(int)\n",
    "        new_features.append('poor_visibility')\n",
    "    \n",
    "    # Interaction terms\n",
    "    if 'max_wind_speed' in weather_features and 'total_rainfall' in weather_features:\n",
    "        df_engineered['wind_rain_interaction'] = df_engineered['max_wind_speed'] * df_engineered['total_rainfall']\n",
    "        new_features.append('wind_rain_interaction')\n",
    "    \n",
    "    if 'avg_temperature' in weather_features and 'avg_humidity' in weather_features:\n",
    "        df_engineered['heat_humidity_index'] = df_engineered['avg_temperature'] * df_engineered['avg_humidity'] / 100\n",
    "        new_features.append('heat_humidity_index')\n",
    "    \n",
    "    print(f\"Engineered {len(new_features)} new features: {new_features}\")\n",
    "    \n",
    "    return df_engineered, new_features\n",
    "\n",
    "# Apply preprocessing\n",
    "print(\"Identifying feature and target columns...\")\n",
    "weather_features, casualty_targets = identify_feature_columns(df)\n",
    "\n",
    "print(f\"Weather features ({len(weather_features)}): {weather_features}\")\n",
    "print(f\"Casualty targets ({len(casualty_targets)}): {casualty_targets}\")\n",
    "\n",
    "# Handle missing values\n",
    "print(\"\\nHandling missing values...\")\n",
    "df_clean = handle_missing_values(df, weather_features)\n",
    "\n",
    "# Engineer features\n",
    "print(\"\\nEngineering weather features...\")\n",
    "df_processed, new_features = engineer_weather_features(df_clean, weather_features)\n",
    "\n",
    "# Update feature list\n",
    "all_weather_features = weather_features + new_features\n",
    "print(f\"\\nTotal weather features: {len(all_weather_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57574f91",
   "metadata": {},
   "source": [
    "## 4. Feature-Target Separation and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9682e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets\n",
    "X = df_processed[all_weather_features]\n",
    "y = df_processed[casualty_targets]\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target matrix shape: {y.shape}\")\n",
    "\n",
    "# Display target statistics\n",
    "print(\"\\nTarget Variable Statistics:\")\n",
    "print(y.describe())\n",
    "\n",
    "# Check for any remaining missing values\n",
    "print(\"\\nMissing values check:\")\n",
    "print(f\"Features missing values: {X.isnull().sum().sum()}\")\n",
    "print(f\"Targets missing values: {y.isnull().sum().sum()}\")\n",
    "\n",
    "# Display correlation between targets\n",
    "print(\"\\nCorrelation between target variables:\")\n",
    "target_corr = y.corr()\n",
    "print(target_corr.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66945909",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set - Features: {X_train.shape}, Targets: {y_train.shape}\")\n",
    "print(f\"Testing set - Features: {X_test.shape}, Targets: {y_test.shape}\")\n",
    "print(f\"Train-test split ratio: {len(X_train)}/{len(X_test)} = {len(X_train)/len(X_test):.2f}\")\n",
    "\n",
    "# Display training set target statistics\n",
    "print(\"\\nTraining set target statistics:\")\n",
    "print(y_train.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8814b0",
   "metadata": {},
   "source": [
    "## 6. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9894d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on training data only\n",
    "print(\"Fitting scaler on training weather features...\")\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform both training and testing data\n",
    "print(\"Scaling training and testing weather features...\")\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrames for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns, index=X_test.index)\n",
    "\n",
    "print(f\"Scaled training features shape: {X_train_scaled.shape}\")\n",
    "print(f\"Scaled testing features shape: {X_test_scaled.shape}\")\n",
    "\n",
    "# Show scaling statistics\n",
    "print(\"\\nScaling validation (training data):\")\n",
    "print(f\"Original features - Mean: {X_train.mean().mean():.4f}, Std: {X_train.std().mean():.4f}\")\n",
    "print(f\"Scaled features - Mean: {X_train_scaled.mean().mean():.4f}, Std: {X_train_scaled.std().mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f721b6d6",
   "metadata": {},
   "source": [
    "## 7. Multi-Output XGBoost Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5eeab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure XGBoost base estimator\n",
    "xgb_estimator = XGBRegressor(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbosity=0  # Suppress XGBoost output\n",
    ")\n",
    "\n",
    "# Create multi-output regressor\n",
    "print(\"Initializing Multi-Output XGBoost Regressor...\")\n",
    "multi_output_model = MultiOutputRegressor(xgb_estimator)\n",
    "\n",
    "# Train the model\n",
    "print(\"Training multi-output model...\")\n",
    "print(\"This may take a few minutes depending on dataset size...\")\n",
    "\n",
    "multi_output_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Model training completed successfully!\")\n",
    "\n",
    "# Display model information\n",
    "print(f\"\\nModel Information:\")\n",
    "print(f\"Number of outputs: {len(multi_output_model.estimators_)}\")\n",
    "print(f\"Output targets: {list(y.columns)}\")\n",
    "print(f\"Number of features: {X_train_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299bfe32",
   "metadata": {},
   "source": [
    "## optuna hyperparmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72f02a",
   "metadata": {},
   "source": [
    "## 8. Model Prediction and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250c2812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(\"Generating predictions...\")\n",
    "y_train_pred = multi_output_model.predict(X_train_scaled)\n",
    "y_test_pred = multi_output_model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions to DataFrames\n",
    "y_train_pred_df = pd.DataFrame(y_train_pred, columns=y.columns, index=y_train.index)\n",
    "y_test_pred_df = pd.DataFrame(y_test_pred, columns=y.columns, index=y_test.index)\n",
    "\n",
    "print(\"Predictions generated successfully!\")\n",
    "print(f\"Training predictions shape: {y_train_pred_df.shape}\")\n",
    "print(f\"Testing predictions shape: {y_test_pred_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c70c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_multi_output_metrics(y_true, y_pred, dataset_name=\"Dataset\"):\n",
    "    \"\"\"Calculate comprehensive metrics for multi-output regression\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, col in enumerate(y_true.columns):\n",
    "        # Extract true and predicted values for this output\n",
    "        y_true_col = y_true.iloc[:, i] if isinstance(y_true, pd.DataFrame) else y_true[:, i]\n",
    "        y_pred_col = y_pred.iloc[:, i] if isinstance(y_pred, pd.DataFrame) else y_pred[:, i]\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_true_col, y_pred_col)\n",
    "        mse = mean_squared_error(y_true_col, y_pred_col)\n",
    "        rmse = np.sqrt(mse)\n",
    "        \n",
    "        results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Output': col,\n",
    "            'MAE': mae,\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Calculate metrics for training and testing sets\n",
    "print(\"Calculating evaluation metrics...\")\n",
    "\n",
    "train_metrics = calculate_multi_output_metrics(y_train, y_train_pred_df, \"Training\")\n",
    "test_metrics = calculate_multi_output_metrics(y_test, y_test_pred_df, \"Testing\")\n",
    "\n",
    "# Combine results\n",
    "all_metrics = pd.concat([train_metrics, test_metrics], ignore_index=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-OUTPUT REGRESSION EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(all_metrics.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba972f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary results table\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY RESULTS TABLE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create pivot table for better visualization\n",
    "summary_table = test_metrics[['Output', 'MAE', 'RMSE']].copy()\n",
    "summary_table = summary_table.round(4)\n",
    "\n",
    "print(\"Test Set Performance by Output:\")\n",
    "print(summary_table.to_string(index=False))\n",
    "\n",
    "# Calculate overall averages\n",
    "avg_mae = test_metrics['MAE'].mean()\n",
    "avg_rmse = test_metrics['RMSE'].mean()\n",
    "\n",
    "print(f\"\\nOverall Averages (Test Set):\")\n",
    "print(f\"Average MAE: {avg_mae:.4f}\")\n",
    "print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
    "\n",
    "# Performance assessment\n",
    "print(f\"\\nModel Performance Assessment:\")\n",
    "for idx, row in test_metrics.iterrows():\n",
    "    output = row['Output']\n",
    "    rmse = row['RMSE']\n",
    "    mae = row['MAE']\n",
    "    \n",
    "    # Get target statistics for context\n",
    "    target_mean = y_test[output].mean()\n",
    "    target_std = y_test[output].std()\n",
    "    \n",
    "    mae_ratio = mae / target_mean if target_mean > 0 else float('inf')\n",
    "    rmse_ratio = rmse / target_std if target_std > 0 else float('inf')\n",
    "    \n",
    "    print(f\"- {output}: MAE/Mean = {mae_ratio:.3f}, RMSE/Std = {rmse_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9aa43de",
   "metadata": {},
   "source": [
    "## 10. Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7ba17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create visualization plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Multi-Output Regression Model Performance', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Actual vs Predicted for each target\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "for i, target in enumerate(casualty_targets):\n",
    "    if i < 4:  # Limit to 4 subplots\n",
    "        row, col = i // 2, i % 2\n",
    "        \n",
    "        y_true_vals = y_test[target].values\n",
    "        y_pred_vals = y_test_pred_df[target].values\n",
    "        \n",
    "        axes[row, col].scatter(y_true_vals, y_pred_vals, alpha=0.6, color=colors[i])\n",
    "        \n",
    "        # Add perfect prediction line\n",
    "        min_val = min(y_true_vals.min(), y_pred_vals.min())\n",
    "        max_val = max(y_true_vals.max(), y_pred_vals.max())\n",
    "        axes[row, col].plot([min_val, max_val], [min_val, max_val], 'r--', lw=2)\n",
    "        \n",
    "        axes[row, col].set_xlabel(f'Actual {target}')\n",
    "        axes[row, col].set_ylabel(f'Predicted {target}')\n",
    "        axes[row, col].set_title(f'{target} - Actual vs Predicted')\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 2: Feature Importance Heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Create feature importance matrix\n",
    "importance_matrix = feature_importance_df.pivot_table(\n",
    "    values='Importance', \n",
    "    index='Feature', \n",
    "    columns='Target', \n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Show only top 20 features\n",
    "top_20_features = avg_importance.head(20).index\n",
    "importance_matrix_top = importance_matrix.loc[top_20_features]\n",
    "\n",
    "sns.heatmap(importance_matrix_top, \n",
    "            annot=True, \n",
    "            fmt='.3f', \n",
    "            cmap='YlOrRd', \n",
    "            cbar_kws={'label': 'Feature Importance'})\n",
    "\n",
    "plt.title('Feature Importance Heatmap (Top 20 Features)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Casualty Targets')\n",
    "plt.ylabel('Weather Features')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555e7d30",
   "metadata": {},
   "source": [
    "## 11. Model Summary and Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc20aeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive model summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MULTI-OUTPUT REGRESSION MODEL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"Dataset Information:\")\n",
    "print(f\"- Total samples: {len(df_processed)}\")\n",
    "print(f\"- Weather features (original): {len(weather_features)}\")\n",
    "print(f\"- Weather features (engineered): {len(all_weather_features)}\")\n",
    "print(f\"- Casualty targets: {len(casualty_targets)}\")\n",
    "print(f\"- Training samples: {len(X_train)}\")\n",
    "print(f\"- Testing samples: {len(X_test)}\")\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"- Algorithm: Multi-Output XGBoost Regression\")\n",
    "print(f\"- Base estimators: {len(multi_output_model.estimators_)}\")\n",
    "print(f\"- XGBoost parameters:\")\n",
    "print(f\"  * n_estimators: 300\")\n",
    "print(f\"  * learning_rate: 0.05\")\n",
    "print(f\"  * max_depth: 5\")\n",
    "print(f\"  * subsample: 0.8\")\n",
    "print(f\"  * colsample_bytree: 0.8\")\n",
    "\n",
    "print(f\"\\nModel Performance (Test Set):\")\n",
    "for idx, row in test_metrics.iterrows():\n",
    "    print(f\"- {row['Output']:<12}: MAE={row['MAE']:<8.4f} RMSE={row['RMSE']:<8.4f}\")\n",
    "\n",
    "print(f\"\\nOverall Performance:\")\n",
    "print(f\"- Average MAE: {avg_mae:.4f}\")\n",
    "print(f\"- Average RMSE: {avg_rmse:.4f}\")\n",
    "\n",
    "# Check for overfitting\n",
    "train_avg_rmse = train_metrics['RMSE'].mean()\n",
    "test_avg_rmse = test_metrics['RMSE'].mean()\n",
    "overfitting_ratio = test_avg_rmse / train_avg_rmse\n",
    "\n",
    "print(f\"\\nGeneralization Assessment:\")\n",
    "print(f\"- Training RMSE: {train_avg_rmse:.4f}\")\n",
    "print(f\"- Testing RMSE: {test_avg_rmse:.4f}\")\n",
    "print(f\"- Generalization ratio: {overfitting_ratio:.3f}\")\n",
    "\n",
    "if overfitting_ratio > 1.2:\n",
    "    print(\"- Warning: Model may be overfitting (test RMSE >> train RMSE)\")\n",
    "elif overfitting_ratio < 0.9:\n",
    "    print(\"- Note: Test performance better than training (possible data leakage?)\")\n",
    "else:\n",
    "    print(\"- Good: Model generalizes well to unseen data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ad80f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model outputs\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# print(\"Saving model outputs...\")\n",
    "\n",
    "# # Save the trained model\n",
    "# model_filename = f'multi_output_xgb_model_{timestamp}.pkl'\n",
    "# with open(model_filename, 'wb') as f:\n",
    "#     pickle.dump(multi_output_model, f)\n",
    "\n",
    "# # Save the scaler\n",
    "# scaler_filename = f'weather_scaler_{timestamp}.pkl'\n",
    "# with open(scaler_filename, 'wb') as f:\n",
    "#     pickle.dump(scaler, f)\n",
    "\n",
    "# # Save predictions\n",
    "# predictions_filename = f'casualty_predictions_{timestamp}.csv'\n",
    "# predictions_output = pd.concat([\n",
    "#     y_test.reset_index(drop=True),\n",
    "#     y_test_pred_df.reset_index(drop=True).add_suffix('_predicted')\n",
    "# ], axis=1)\n",
    "# predictions_output.to_csv(predictions_filename, index=False)\n",
    "\n",
    "# # Save evaluation metrics\n",
    "# metrics_filename = f'model_evaluation_metrics_{timestamp}.csv'\n",
    "# all_metrics.to_csv(metrics_filename, index=False)\n",
    "\n",
    "# # Save feature importance\n",
    "# importance_filename = f'feature_importance_{timestamp}.csv'\n",
    "# feature_importance_df.to_csv(importance_filename, index=False)\n",
    "\n",
    "# print(f\"\\nFiles saved:\")\n",
    "# print(f\"- {model_filename} (trained multi-output model)\")\n",
    "# print(f\"- {scaler_filename} (fitted feature scaler)\")\n",
    "# print(f\"- {predictions_filename} (test predictions vs actual)\")\n",
    "# print(f\"- {metrics_filename} (evaluation metrics)\")\n",
    "# print(f\"- {importance_filename} (feature importance scores)\")\n",
    "\n",
    "# print(f\"\\n\" + \"=\"*80)\n",
    "# print(\"MULTI-OUTPUT REGRESSION PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "# print(\"=\"*80)\n",
    "# print(\"✓ Weather data preprocessing and feature engineering completed\")\n",
    "# print(\"✓ Features scaled and train-test split performed\")\n",
    "# print(\"✓ Multi-output XGBoost model trained successfully\")\n",
    "# print(\"✓ Comprehensive evaluation metrics calculated\")\n",
    "# print(\"✓ Feature importance analysis completed\")\n",
    "# print(\"✓ Results visualized and saved\")\n",
    "# print(f\"✓ Average model RMSE: {avg_rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
