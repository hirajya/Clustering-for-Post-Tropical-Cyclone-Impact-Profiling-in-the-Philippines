{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5e2c4ed",
   "metadata": {},
   "source": [
    "# Multi-Task Neural Network (MTNN) for Typhoon Impact Prediction\n",
    "\n",
    "## OPTION B: Multi-Task Learning with Shared Layers and Multiple Output Heads\n",
    "\n",
    "This notebook implements a Multi-Task Learning (MTL) neural network that uses:\n",
    "- **Shared dense layers** for feature extraction\n",
    "- **Multiple output heads** for different prediction tasks:\n",
    "  - **Regression heads**: families, person, brgy, cost, partially, totally\n",
    "  - **Classification heads**: dead, injured_ill, missing (binary labels)\n",
    "\n",
    "### Benefits of Multi-Task Learning:\n",
    "1. Shared representations improve generalization\n",
    "2. Regularization effect from auxiliary tasks\n",
    "3. Efficient parameter sharing\n",
    "4. Better handling of rare events through joint learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf12bc",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db22dfa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (4.13.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Collecting numpy<2.2.0,>=1.26.0\n",
      "  Using cached numpy-2.1.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\rodney lei estrada\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.25.2\n",
      "    Uninstalling numpy-1.25.2:\n",
      "      Successfully uninstalled numpy-1.25.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\Rodney Lei Estrada\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\Lib\\\\site-packages\\\\~umpy\\\\.libs\\\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cfc3863",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\Rodney Lei Estrada\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     mean_absolute_error, mean_squared_error, r2_score,\n\u001b[32m     18\u001b[39m     accuracy_score, f1_score, roc_auc_score, confusion_matrix,\n\u001b[32m     19\u001b[39m     classification_report\n\u001b[32m     20\u001b[39m )\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# TensorFlow/Keras\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers, Model, Input\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[39m\n\u001b[32m     86\u001b[39m     sys.setdlopenflags(_default_dlopen_flags)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     89\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback.format_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     90\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     91\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     92\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     93\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIf you need help, create an issue \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     94\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     95\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mand include the entire stack trace above this error message.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: Traceback (most recent call last):\n  File \"C:\\Users\\Rodney Lei Estrada\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
     ]
    }
   ],
   "source": [
    "# Core Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    accuracy_score, f1_score, roc_auc_score, confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "# TensorFlow/Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8907fcd",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14f700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = '../data/typhoon_impact_with_extreme_weather.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Display basic information\n",
    "print(\"Dataset Shape:\", df.shape)\n",
    "print(\"\\nColumn Names:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d69ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types and missing values\n",
    "print(\"Data Types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6c4e54",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998410e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input features and target columns\n",
    "INPUT_FEATURES = [\n",
    "    'Max_Sustained_Wind_kph',\n",
    "    'Typhoon_Type',\n",
    "    'Max_24hr_Rainfall_mm',\n",
    "    'Total_Storm_Rainfall_mm',\n",
    "    'Min_Pressure_hPa'\n",
    "]\n",
    "\n",
    "# Regression targets\n",
    "REGRESSION_TARGETS = ['Families', 'Person', 'Brgy', 'Cost', 'Partially', 'Totally']\n",
    "\n",
    "# Classification targets (will be converted to binary)\n",
    "CLASSIFICATION_TARGETS = ['Dead', 'Injured/Ill', 'Missing']\n",
    "\n",
    "# Clean column names for output (lowercase, no special chars)\n",
    "REGRESSION_OUTPUT_NAMES = ['families', 'person', 'brgy', 'cost', 'partially', 'totally']\n",
    "CLASSIFICATION_OUTPUT_NAMES = ['dead', 'injured_ill', 'missing']\n",
    "\n",
    "print(\"Input Features:\", INPUT_FEATURES)\n",
    "print(\"\\nRegression Targets:\", REGRESSION_TARGETS)\n",
    "print(\"\\nClassification Targets:\", CLASSIFICATION_TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f8a4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a working copy of the dataframe\n",
    "df_model = df.copy()\n",
    "\n",
    "# Handle missing values in input features\n",
    "for col in INPUT_FEATURES:\n",
    "    if col in df_model.columns:\n",
    "        if df_model[col].dtype == 'object':\n",
    "            df_model[col] = df_model[col].fillna(df_model[col].mode()[0])\n",
    "        else:\n",
    "            df_model[col] = df_model[col].fillna(df_model[col].median())\n",
    "\n",
    "# Handle missing values in target columns\n",
    "for col in REGRESSION_TARGETS + CLASSIFICATION_TARGETS:\n",
    "    if col in df_model.columns:\n",
    "        df_model[col] = df_model[col].fillna(0)\n",
    "\n",
    "print(\"Missing values after imputation:\")\n",
    "print(df_model[INPUT_FEATURES + REGRESSION_TARGETS + CLASSIFICATION_TARGETS].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445eaa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical feature: Typhoon_Type\n",
    "label_encoder = LabelEncoder()\n",
    "df_model['Typhoon_Type_Encoded'] = label_encoder.fit_transform(df_model['Typhoon_Type'])\n",
    "\n",
    "# Display typhoon type mapping\n",
    "typhoon_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "print(\"Typhoon Type Encoding:\")\n",
    "for k, v in typhoon_mapping.items():\n",
    "    print(f\"  {k}: {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4715db3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary classification labels (value > 0)\n",
    "df_model['Dead_Binary'] = (df_model['Dead'] > 0).astype(int)\n",
    "df_model['Injured_Ill_Binary'] = (df_model['Injured/Ill'] > 0).astype(int)\n",
    "df_model['Missing_Binary'] = (df_model['Missing'] > 0).astype(int)\n",
    "\n",
    "# Check class distribution for binary targets\n",
    "print(\"Binary Classification Label Distribution:\")\n",
    "print(f\"\\nDead (>0): {df_model['Dead_Binary'].sum()} / {len(df_model)} ({df_model['Dead_Binary'].mean()*100:.2f}%)\")\n",
    "print(f\"Injured/Ill (>0): {df_model['Injured_Ill_Binary'].sum()} / {len(df_model)} ({df_model['Injured_Ill_Binary'].mean()*100:.2f}%)\")\n",
    "print(f\"Missing (>0): {df_model['Missing_Binary'].sum()} / {len(df_model)} ({df_model['Missing_Binary'].mean()*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7addf1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix X\n",
    "feature_columns = [\n",
    "    'Max_Sustained_Wind_kph',\n",
    "    'Typhoon_Type_Encoded',\n",
    "    'Max_24hr_Rainfall_mm',\n",
    "    'Total_Storm_Rainfall_mm',\n",
    "    'Min_Pressure_hPa'\n",
    "]\n",
    "\n",
    "X = df_model[feature_columns].values\n",
    "\n",
    "# Prepare target dictionaries\n",
    "# Regression targets\n",
    "y_regression = {\n",
    "    'families': df_model['Families'].values,\n",
    "    'person': df_model['Person'].values,\n",
    "    'brgy': df_model['Brgy'].values,\n",
    "    'cost': df_model['Cost'].values,\n",
    "    'partially': df_model['Partially'].values,\n",
    "    'totally': df_model['Totally'].values\n",
    "}\n",
    "\n",
    "# Classification targets (binary)\n",
    "y_classification = {\n",
    "    'dead': df_model['Dead_Binary'].values,\n",
    "    'injured_ill': df_model['Injured_Ill_Binary'].values,\n",
    "    'missing': df_model['Missing_Binary'].values\n",
    "}\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Number of regression targets: {len(y_regression)}\")\n",
    "print(f\"Number of classification targets: {len(y_classification)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8e4660",
   "metadata": {},
   "source": [
    "## 4. Train/Test Split and Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78750a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all targets for splitting\n",
    "y_all = {**y_regression, **y_classification}\n",
    "\n",
    "# Create indices for splitting\n",
    "indices = np.arange(len(X))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split features\n",
    "X_train = X[train_idx]\n",
    "X_test = X[test_idx]\n",
    "\n",
    "# Split targets\n",
    "y_train = {key: val[train_idx] for key, val in y_all.items()}\n",
    "y_test = {key: val[test_idx] for key, val in y_all.items()}\n",
    "\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Test set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1ee041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale input features\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Scale regression targets for better training\n",
    "scalers_y = {}\n",
    "y_train_scaled = {}\n",
    "y_test_scaled = {}\n",
    "\n",
    "for target in REGRESSION_OUTPUT_NAMES:\n",
    "    scaler = StandardScaler()\n",
    "    y_train_scaled[target] = scaler.fit_transform(y_train[target].reshape(-1, 1)).flatten()\n",
    "    y_test_scaled[target] = scaler.transform(y_test[target].reshape(-1, 1)).flatten()\n",
    "    scalers_y[target] = scaler\n",
    "\n",
    "# Classification targets don't need scaling\n",
    "for target in CLASSIFICATION_OUTPUT_NAMES:\n",
    "    y_train_scaled[target] = y_train[target]\n",
    "    y_test_scaled[target] = y_test[target]\n",
    "\n",
    "print(\"Feature and target scaling complete.\")\n",
    "print(f\"\\nScaled X_train stats: mean={X_train_scaled.mean():.4f}, std={X_train_scaled.std():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb03a5c",
   "metadata": {},
   "source": [
    "## 5. Build Multi-Task Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81198bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mtl_model(input_dim, regression_targets, classification_targets):\n",
    "    \"\"\"\n",
    "    Build a Multi-Task Learning model with shared layers and multiple output heads.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input Layer\n",
    "    - Shared Dense Layers (128 -> 64 -> 32)\n",
    "    - Separate output heads for each target\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    input_dim : int\n",
    "        Number of input features\n",
    "    regression_targets : list\n",
    "        List of regression target names\n",
    "    classification_targets : list\n",
    "        List of classification target names\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    model : keras.Model\n",
    "        Compiled MTL model\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input layer\n",
    "    inputs = Input(shape=(input_dim,), name='input')\n",
    "    \n",
    "    # Shared layers\n",
    "    shared = layers.Dense(128, activation='relu', name='shared_dense_1')(inputs)\n",
    "    shared = layers.BatchNormalization(name='shared_bn_1')(shared)\n",
    "    shared = layers.Dropout(0.3, name='shared_dropout_1')(shared)\n",
    "    \n",
    "    shared = layers.Dense(64, activation='relu', name='shared_dense_2')(shared)\n",
    "    shared = layers.BatchNormalization(name='shared_bn_2')(shared)\n",
    "    shared = layers.Dropout(0.2, name='shared_dropout_2')(shared)\n",
    "    \n",
    "    shared = layers.Dense(32, activation='relu', name='shared_dense_3')(shared)\n",
    "    shared = layers.BatchNormalization(name='shared_bn_3')(shared)\n",
    "    shared = layers.Dropout(0.1, name='shared_dropout_3')(shared)\n",
    "    \n",
    "    # Output heads dictionary\n",
    "    outputs = {}\n",
    "    \n",
    "    # Regression output heads\n",
    "    for target in regression_targets:\n",
    "        # Task-specific layer\n",
    "        task_layer = layers.Dense(16, activation='relu', name=f'{target}_dense')(shared)\n",
    "        # Output: linear activation for regression\n",
    "        outputs[target] = layers.Dense(1, activation='linear', name=target)(task_layer)\n",
    "    \n",
    "    # Classification output heads\n",
    "    for target in classification_targets:\n",
    "        # Task-specific layer\n",
    "        task_layer = layers.Dense(16, activation='relu', name=f'{target}_dense')(shared)\n",
    "        # Output: sigmoid activation for binary classification\n",
    "        outputs[target] = layers.Dense(1, activation='sigmoid', name=target)(task_layer)\n",
    "    \n",
    "    # Create model\n",
    "    model = Model(inputs=inputs, outputs=outputs, name='MTL_Model')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build the model\n",
    "model = build_mtl_model(\n",
    "    input_dim=X_train_scaled.shape[1],\n",
    "    regression_targets=REGRESSION_OUTPUT_NAMES,\n",
    "    classification_targets=CLASSIFICATION_OUTPUT_NAMES\n",
    ")\n",
    "\n",
    "# Display model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f59abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model architecture\n",
    "try:\n",
    "    tf.keras.utils.plot_model(\n",
    "        model,\n",
    "        to_file='../models/mtl_model_architecture.png',\n",
    "        show_shapes=True,\n",
    "        show_layer_names=True,\n",
    "        rankdir='TB',\n",
    "        dpi=100\n",
    "    )\n",
    "    print(\"Model architecture saved to '../models/mtl_model_architecture.png'\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not save model architecture image: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa5c261",
   "metadata": {},
   "source": [
    "## 6. Define Loss Functions and Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dce907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define losses for each output head\n",
    "losses = {\n",
    "    # Regression losses (MSE)\n",
    "    'families': 'mse',\n",
    "    'person': 'mse',\n",
    "    'brgy': 'mse',\n",
    "    'cost': 'mse',\n",
    "    'partially': 'mse',\n",
    "    'totally': 'mse',\n",
    "    # Classification losses (Binary Cross-Entropy)\n",
    "    'dead': 'binary_crossentropy',\n",
    "    'injured_ill': 'binary_crossentropy',\n",
    "    'missing': 'binary_crossentropy'\n",
    "}\n",
    "\n",
    "# Define loss weights to balance regression vs classification\n",
    "# Classification tasks are given higher weight due to class imbalance\n",
    "loss_weights = {\n",
    "    # Regression weights\n",
    "    'families': 1.0,\n",
    "    'person': 1.0,\n",
    "    'brgy': 1.0,\n",
    "    'cost': 0.5,  # Cost can be very large, reduce weight\n",
    "    'partially': 1.0,\n",
    "    'totally': 1.0,\n",
    "    # Classification weights (higher for rare events)\n",
    "    'dead': 5.0,\n",
    "    'injured_ill': 3.0,\n",
    "    'missing': 4.0\n",
    "}\n",
    "\n",
    "# Define metrics for each output\n",
    "metrics = {\n",
    "    'families': ['mae'],\n",
    "    'person': ['mae'],\n",
    "    'brgy': ['mae'],\n",
    "    'cost': ['mae'],\n",
    "    'partially': ['mae'],\n",
    "    'totally': ['mae'],\n",
    "    'dead': ['accuracy', 'AUC'],\n",
    "    'injured_ill': ['accuracy', 'AUC'],\n",
    "    'missing': ['accuracy', 'AUC']\n",
    "}\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss=losses,\n",
    "    loss_weights=loss_weights,\n",
    "    metrics=metrics\n",
    ")\n",
    "\n",
    "print(\"Model compiled successfully!\")\n",
    "print(\"\\nLoss functions:\")\n",
    "for k, v in losses.items():\n",
    "    print(f\"  {k}: {v} (weight: {loss_weights[k]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3895ae",
   "metadata": {},
   "source": [
    "## 7. Train the MTL Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2143a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=15,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-6,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "print(\"Training Multi-Task Learning Model...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled,\n",
    "    y_train_scaled,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9500e16a",
   "metadata": {},
   "source": [
    "## 8. Visualize Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15dab8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss curves.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # Plot total loss\n",
    "    ax = axes[0, 0]\n",
    "    ax.plot(history.history['loss'], label='Training Loss', linewidth=2)\n",
    "    ax.plot(history.history['val_loss'], label='Validation Loss', linewidth=2)\n",
    "    ax.set_title('Total Loss', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot regression losses\n",
    "    ax = axes[0, 1]\n",
    "    for target in REGRESSION_OUTPUT_NAMES:\n",
    "        if f'{target}_loss' in history.history:\n",
    "            ax.plot(history.history[f'{target}_loss'], label=f'{target}', alpha=0.7)\n",
    "    ax.set_title('Regression Losses (Training)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss (MSE)')\n",
    "    ax.legend(loc='upper right', fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot classification losses\n",
    "    ax = axes[1, 0]\n",
    "    for target in CLASSIFICATION_OUTPUT_NAMES:\n",
    "        if f'{target}_loss' in history.history:\n",
    "            ax.plot(history.history[f'{target}_loss'], label=f'{target}', linewidth=2)\n",
    "    ax.set_title('Classification Losses (Training)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss (BCE)')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot classification AUC\n",
    "    ax = axes[1, 1]\n",
    "    for target in CLASSIFICATION_OUTPUT_NAMES:\n",
    "        auc_key = f'{target}_auc'\n",
    "        # Handle different naming conventions in Keras\n",
    "        for key in history.history.keys():\n",
    "            if target in key.lower() and 'auc' in key.lower() and 'val' not in key.lower():\n",
    "                ax.plot(history.history[key], label=f'{target} (train)', linewidth=2)\n",
    "                break\n",
    "    ax.set_title('Classification AUC (Training)', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('AUC')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../models/mtl_training_history.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Training history plot saved to '../models/mtl_training_history.png'\")\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0f0a10",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8562e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "predictions = model.predict(X_test_scaled)\n",
    "\n",
    "# Convert predictions to dictionary if not already\n",
    "if isinstance(predictions, list):\n",
    "    output_names = REGRESSION_OUTPUT_NAMES + CLASSIFICATION_OUTPUT_NAMES\n",
    "    predictions = {name: pred.flatten() for name, pred in zip(output_names, predictions)}\n",
    "\n",
    "print(\"Predictions generated for test set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e886e6b",
   "metadata": {},
   "source": [
    "### 9.1 Regression Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917414aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(y_true, y_pred, target_name, scaler=None):\n",
    "    \"\"\"\n",
    "    Evaluate regression performance.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing MAE, RMSE, and RÂ² scores\n",
    "    \"\"\"\n",
    "    # Inverse transform if scaler provided\n",
    "    if scaler is not None:\n",
    "        y_true_orig = scaler.inverse_transform(y_true.reshape(-1, 1)).flatten()\n",
    "        y_pred_orig = scaler.inverse_transform(y_pred.reshape(-1, 1)).flatten()\n",
    "    else:\n",
    "        y_true_orig = y_true\n",
    "        y_pred_orig = y_pred\n",
    "    \n",
    "    mae = mean_absolute_error(y_true_orig, y_pred_orig)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))\n",
    "    r2 = r2_score(y_true_orig, y_pred_orig)\n",
    "    \n",
    "    return {\n",
    "        'Target': target_name,\n",
    "        'MAE': mae,\n",
    "        'RMSE': rmse,\n",
    "        'RÂ²': r2\n",
    "    }, y_true_orig, y_pred_orig\n",
    "\n",
    "# Evaluate all regression targets\n",
    "regression_results = []\n",
    "regression_predictions = {}\n",
    "\n",
    "print(\"Regression Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for target in REGRESSION_OUTPUT_NAMES:\n",
    "    y_true = y_test_scaled[target]\n",
    "    y_pred = predictions[target].flatten()\n",
    "    \n",
    "    result, y_true_orig, y_pred_orig = evaluate_regression(\n",
    "        y_true, y_pred, target, scalers_y.get(target)\n",
    "    )\n",
    "    regression_results.append(result)\n",
    "    regression_predictions[target] = {'true': y_true_orig, 'pred': y_pred_orig}\n",
    "\n",
    "# Display results as DataFrame\n",
    "df_regression_results = pd.DataFrame(regression_results)\n",
    "print(df_regression_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c4ea0",
   "metadata": {},
   "source": [
    "### 9.2 Classification Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68222c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classification(y_true, y_pred_proba, target_name, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Evaluate classification performance.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing Accuracy, F1, and ROC-AUC scores\n",
    "    \"\"\"\n",
    "    y_pred = (y_pred_proba >= threshold).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    # Handle cases where only one class is present\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    except ValueError:\n",
    "        roc_auc = np.nan\n",
    "    \n",
    "    return {\n",
    "        'Target': target_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    }, y_pred\n",
    "\n",
    "# Evaluate all classification targets\n",
    "classification_results = []\n",
    "classification_predictions = {}\n",
    "\n",
    "print(\"\\nClassification Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for target in CLASSIFICATION_OUTPUT_NAMES:\n",
    "    y_true = y_test_scaled[target]\n",
    "    y_pred_proba = predictions[target].flatten()\n",
    "    \n",
    "    result, y_pred = evaluate_classification(y_true, y_pred_proba, target)\n",
    "    classification_results.append(result)\n",
    "    classification_predictions[target] = {\n",
    "        'true': y_true,\n",
    "        'pred': y_pred,\n",
    "        'proba': y_pred_proba\n",
    "    }\n",
    "\n",
    "# Display results as DataFrame\n",
    "df_classification_results = pd.DataFrame(classification_results)\n",
    "print(df_classification_results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9f8db6",
   "metadata": {},
   "source": [
    "## 10. Visualization: Actual vs Predicted (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40742887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_actual_vs_predicted(regression_predictions, save_path=None):\n",
    "    \"\"\"\n",
    "    Create actual vs predicted scatter plots for regression targets.\n",
    "    \"\"\"\n",
    "    n_targets = len(regression_predictions)\n",
    "    n_cols = 3\n",
    "    n_rows = (n_targets + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for idx, (target, data) in enumerate(regression_predictions.items()):\n",
    "        ax = axes[idx]\n",
    "        y_true = data['true']\n",
    "        y_pred = data['pred']\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax.scatter(y_true, y_pred, alpha=0.5, edgecolors='none', s=30)\n",
    "        \n",
    "        # Perfect prediction line\n",
    "        min_val = min(y_true.min(), y_pred.min())\n",
    "        max_val = max(y_true.max(), y_pred.max())\n",
    "        ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "        \n",
    "        # Calculate RÂ²\n",
    "        r2 = r2_score(y_true, y_pred)\n",
    "        \n",
    "        ax.set_xlabel('Actual Values', fontsize=10)\n",
    "        ax.set_ylabel('Predicted Values', fontsize=10)\n",
    "        ax.set_title(f'{target.upper()}\\n(RÂ² = {r2:.4f})', fontsize=11, fontweight='bold')\n",
    "        ax.legend(loc='upper left', fontsize=8)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for idx in range(n_targets, len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Plot saved to '{save_path}'\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot actual vs predicted for regression targets\n",
    "plot_actual_vs_predicted(\n",
    "    regression_predictions,\n",
    "    save_path='../models/mtl_actual_vs_predicted.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c3a016",
   "metadata": {},
   "source": [
    "## 11. Visualization: Confusion Matrices (Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb49e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(classification_predictions, save_path=None):\n",
    "    \"\"\"\n",
    "    Create confusion matrix heatmaps for classification targets.\n",
    "    \"\"\"\n",
    "    n_targets = len(classification_predictions)\n",
    "    fig, axes = plt.subplots(1, n_targets, figsize=(5*n_targets, 4))\n",
    "    \n",
    "    if n_targets == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (target, data) in enumerate(classification_predictions.items()):\n",
    "        ax = axes[idx]\n",
    "        y_true = data['true']\n",
    "        y_pred = data['pred']\n",
    "        \n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        \n",
    "        # Plot heatmap\n",
    "        sns.heatmap(\n",
    "            cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=['Negative (0)', 'Positive (1)'],\n",
    "            yticklabels=['Negative (0)', 'Positive (1)'],\n",
    "            ax=ax, cbar=True\n",
    "        )\n",
    "        \n",
    "        ax.set_xlabel('Predicted', fontsize=10)\n",
    "        ax.set_ylabel('Actual', fontsize=10)\n",
    "        ax.set_title(f'{target.upper()}\\nConfusion Matrix', fontsize=11, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Plot saved to '{save_path}'\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot confusion matrices\n",
    "plot_confusion_matrices(\n",
    "    classification_predictions,\n",
    "    save_path='../models/mtl_confusion_matrices.png'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a347be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print detailed classification reports\n",
    "print(\"\\nDetailed Classification Reports\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for target in CLASSIFICATION_OUTPUT_NAMES:\n",
    "    print(f\"\\n{target.upper()}:\")\n",
    "    print(\"-\" * 40)\n",
    "    y_true = classification_predictions[target]['true']\n",
    "    y_pred = classification_predictions[target]['pred']\n",
    "    print(classification_report(y_true, y_pred, target_names=['No Event', 'Event'], zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8c6f6a",
   "metadata": {},
   "source": [
    "## 12. Summary of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44bb4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a comprehensive summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MULTI-TASK NEURAL NETWORK - EVALUATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nðŸ“Š REGRESSION PERFORMANCE:\")\n",
    "print(\"-\"*50)\n",
    "print(df_regression_results.to_string(index=False))\n",
    "\n",
    "print(\"\\nðŸ“ˆ CLASSIFICATION PERFORMANCE:\")\n",
    "print(\"-\"*50)\n",
    "print(df_classification_results.to_string(index=False))\n",
    "\n",
    "# Calculate average metrics\n",
    "avg_r2 = df_regression_results['RÂ²'].mean()\n",
    "avg_mae = df_regression_results['MAE'].mean()\n",
    "avg_accuracy = df_classification_results['Accuracy'].mean()\n",
    "avg_f1 = df_classification_results['F1 Score'].mean()\n",
    "avg_auc = df_classification_results['ROC-AUC'].mean()\n",
    "\n",
    "print(\"\\nðŸ“‹ AVERAGE METRICS:\")\n",
    "print(\"-\"*50)\n",
    "print(f\"  Regression Average RÂ²: {avg_r2:.4f}\")\n",
    "print(f\"  Regression Average MAE: {avg_mae:.4f}\")\n",
    "print(f\"  Classification Average Accuracy: {avg_accuracy:.4f}\")\n",
    "print(f\"  Classification Average F1 Score: {avg_f1:.4f}\")\n",
    "print(f\"  Classification Average ROC-AUC: {avg_auc:.4f}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8265acd6",
   "metadata": {},
   "source": [
    "## 13. Save the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb70c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# Save the model in H5 format\n",
    "model_path = '../models/mtl_typhoon_prediction_model.h5'\n",
    "model.save(model_path)\n",
    "print(f\"âœ… Model saved to: {model_path}\")\n",
    "\n",
    "# Save the model in Keras format as well\n",
    "model_keras_path = '../models/mtl_typhoon_prediction_model.keras'\n",
    "model.save(model_keras_path)\n",
    "print(f\"âœ… Model saved to: {model_keras_path}\")\n",
    "\n",
    "# Save scalers for future inference\n",
    "scalers_path = '../models/mtl_scalers.joblib'\n",
    "joblib.dump({\n",
    "    'scaler_X': scaler_X,\n",
    "    'scalers_y': scalers_y,\n",
    "    'label_encoder': label_encoder\n",
    "}, scalers_path)\n",
    "print(f\"âœ… Scalers saved to: {scalers_path}\")\n",
    "\n",
    "# Save evaluation results\n",
    "results_path = '../models/mtl_evaluation_results.joblib'\n",
    "joblib.dump({\n",
    "    'regression_results': df_regression_results,\n",
    "    'classification_results': df_classification_results,\n",
    "    'training_history': history.history\n",
    "}, results_path)\n",
    "print(f\"âœ… Evaluation results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32be421",
   "metadata": {},
   "source": [
    "## 14. Model Inference Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee9b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_typhoon_impact(model, scaler_X, scalers_y, label_encoder, typhoon_data):\n",
    "    \"\"\"\n",
    "    Make predictions for new typhoon data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : keras.Model\n",
    "        Trained MTL model\n",
    "    scaler_X : StandardScaler\n",
    "        Feature scaler\n",
    "    scalers_y : dict\n",
    "        Target scalers for regression outputs\n",
    "    label_encoder : LabelEncoder\n",
    "        Typhoon type encoder\n",
    "    typhoon_data : dict\n",
    "        Dictionary with typhoon features\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Predictions for all targets\n",
    "    \"\"\"\n",
    "    # Encode typhoon type\n",
    "    typhoon_type_encoded = label_encoder.transform([typhoon_data['typhoon_type']])[0]\n",
    "    \n",
    "    # Create feature vector\n",
    "    X_new = np.array([[\n",
    "        typhoon_data['max_sustained_wind_kph'],\n",
    "        typhoon_type_encoded,\n",
    "        typhoon_data['max_24hr_rainfall_mm'],\n",
    "        typhoon_data['total_storm_rainfall_mm'],\n",
    "        typhoon_data['min_pressure_hpa']\n",
    "    ]])\n",
    "    \n",
    "    # Scale features\n",
    "    X_new_scaled = scaler_X.transform(X_new)\n",
    "    \n",
    "    # Get predictions\n",
    "    predictions = model.predict(X_new_scaled, verbose=0)\n",
    "    \n",
    "    # Process predictions\n",
    "    results = {}\n",
    "    \n",
    "    # Regression predictions (inverse transform)\n",
    "    for i, target in enumerate(REGRESSION_OUTPUT_NAMES):\n",
    "        pred_scaled = predictions[target][0][0]\n",
    "        pred_original = scalers_y[target].inverse_transform([[pred_scaled]])[0][0]\n",
    "        results[target] = max(0, pred_original)  # Ensure non-negative\n",
    "    \n",
    "    # Classification predictions\n",
    "    for target in CLASSIFICATION_OUTPUT_NAMES:\n",
    "        proba = predictions[target][0][0]\n",
    "        results[f'{target}_probability'] = proba\n",
    "        results[f'{target}_prediction'] = 'Yes' if proba >= 0.5 else 'No'\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Example prediction\n",
    "example_typhoon = {\n",
    "    'max_sustained_wind_kph': 195,\n",
    "    'typhoon_type': 'STY',  # Super Typhoon\n",
    "    'max_24hr_rainfall_mm': 300,\n",
    "    'total_storm_rainfall_mm': 450,\n",
    "    'min_pressure_hpa': 940\n",
    "}\n",
    "\n",
    "print(\"\\nðŸŒ€ EXAMPLE PREDICTION\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\nInput Typhoon Data:\")\n",
    "for k, v in example_typhoon.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Make prediction\n",
    "prediction_results = predict_typhoon_impact(\n",
    "    model, scaler_X, scalers_y, label_encoder, example_typhoon\n",
    ")\n",
    "\n",
    "print(\"\\nPredicted Impact:\")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nRegression Predictions:\")\n",
    "for target in REGRESSION_OUTPUT_NAMES:\n",
    "    print(f\"  {target}: {prediction_results[target]:,.2f}\")\n",
    "\n",
    "print(\"\\nClassification Predictions:\")\n",
    "for target in CLASSIFICATION_OUTPUT_NAMES:\n",
    "    prob = prediction_results[f'{target}_probability']\n",
    "    pred = prediction_results[f'{target}_prediction']\n",
    "    print(f\"  {target}: {pred} (probability: {prob:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "672510ee",
   "metadata": {},
   "source": [
    "## 15. Conclusion\n",
    "\n",
    "### Summary\n",
    "\n",
    "This notebook implemented a **Multi-Task Neural Network (MTNN)** for predicting typhoon impacts in the Philippines. The key features include:\n",
    "\n",
    "1. **Shared Feature Learning**: Common dense layers (128 â†’ 64 â†’ 32) extract shared representations from typhoon characteristics.\n",
    "\n",
    "2. **Multiple Output Heads**:\n",
    "   - **6 Regression heads**: Predict continuous values (families, persons, barangays, cost, partially/totally damaged)\n",
    "   - **3 Classification heads**: Predict binary outcomes (casualties: dead, injured/ill, missing)\n",
    "\n",
    "3. **Balanced Training**: Custom loss weights help balance regression and classification tasks, with higher weights for rare events.\n",
    "\n",
    "### Files Generated:\n",
    "- `../models/mtl_typhoon_prediction_model.h5` - Trained model (H5 format)\n",
    "- `../models/mtl_typhoon_prediction_model.keras` - Trained model (Keras format)\n",
    "- `../models/mtl_scalers.joblib` - Feature and target scalers\n",
    "- `../models/mtl_evaluation_results.joblib` - Evaluation metrics\n",
    "- `../models/mtl_training_history.png` - Training curves\n",
    "- `../models/mtl_actual_vs_predicted.png` - Regression scatter plots\n",
    "- `../models/mtl_confusion_matrices.png` - Classification confusion matrices\n",
    "\n",
    "### Next Steps:\n",
    "1. Fine-tune hyperparameters (learning rate, layer sizes, dropout rates)\n",
    "2. Experiment with different architectures (e.g., residual connections)\n",
    "3. Add more features (geographic, temporal)\n",
    "4. Implement class weighting for imbalanced classification targets\n",
    "5. Deploy the model for real-time predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
