{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3faaf15",
   "metadata": {},
   "source": [
    "# Multi-Output Regression for Typhoon Impact Prediction\n",
    "\n",
    "This notebook implements a Multi-Output Regression model to predict typhoon impact metrics based on meteorological features.\n",
    "\n",
    "## Input Features:\n",
    "- `max_sustained_wind_kph`: Maximum sustained wind speed in kph\n",
    "- `typhoon_type`: Type/Category of typhoon\n",
    "- `max_24hr_rainfall_mm`: Maximum 24-hour rainfall in mm\n",
    "- `total_storm_rainfall_mm`: Total storm rainfall in mm\n",
    "- `min_pressure_hpa`: Minimum pressure in hPa\n",
    "\n",
    "## Output Targets:\n",
    "- `families`: Number of affected families\n",
    "- `person`: Number of affected persons\n",
    "- `brgy`: Number of affected barangays\n",
    "- `dead`: Number of deaths\n",
    "- `injured/ill`: Number of injured/ill\n",
    "- `missing`: Number of missing persons\n",
    "- `cost`: Damage cost\n",
    "- `partially`: Partially damaged structures\n",
    "- `totally`: Totally damaged structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ffa487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and numerical libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Scikit-learn components\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# XGBoost for regression\n",
    "import xgboost as xgb\n",
    "\n",
    "# Hyperparameter optimization\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model persistence\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638307b1",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebeb6a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Typhoon Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Year",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Region",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Province",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "City/Municipality",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Families",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Person",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Brgy",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Dead",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Injured/Ill",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Missing",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Totally",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Partially",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Total",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Quantity",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Cost",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Nearest_Station",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Station_Province",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Distance_km",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "PAR_START",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "PAR_END",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Duration_in_PAR_Hours",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Max_Sustained_Wind_kph",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Typhoon_Type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Max_24hr_Rainfall_mm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Total_Storm_Rainfall_mm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Min_Pressure_hPa",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Weather_Station_Mapped",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Weather_Records_Found",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Weather_Days_Covered",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "930e1c47-aa82-47a3-844d-6dad3d1468ac",
       "rows": [
        [
         "0",
         "BETTY",
         "2023",
         "2",
         "BATANES",
         "BASCO",
         "3608",
         "11120",
         "6",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "3608",
         "2646179.36",
         "BASCO",
         "BATANES",
         "2.497503561",
         "5/27/2023",
         "6/1/2023",
         "133.0",
         "195",
         "STY",
         "25.6",
         "39.0",
         "977.1",
         "1",
         "6",
         "6"
        ],
        [
         "1",
         "BETTY",
         "2023",
         "2",
         "BATANES",
         "ITBAYAT",
         "968",
         "3028",
         "5",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "966",
         "494592.0",
         "ITBAYAT",
         "BATANES",
         "3.204942957",
         "5/27/2023",
         "6/1/2023",
         "133.0",
         "195",
         "STY",
         "20.3",
         "32.71",
         "1000.5",
         "1",
         "6",
         "6"
        ],
        [
         "2",
         "BETTY",
         "2023",
         "2",
         "BATANES",
         "IVANA",
         "444",
         "1532",
         "4",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "444",
         "227328.0",
         "BASCO",
         "BATANES",
         "9.470553732",
         "5/27/2023",
         "6/1/2023",
         "133.0",
         "195",
         "STY",
         "25.6",
         "39.0",
         "977.1",
         "1",
         "6",
         "6"
        ],
        [
         "3",
         "BETTY",
         "2023",
         "2",
         "BATANES",
         "MAHATAO",
         "575",
         "1792",
         "4",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "575",
         "291082.96",
         "BASCO",
         "BATANES",
         "4.890815627",
         "5/27/2023",
         "6/1/2023",
         "133.0",
         "195",
         "STY",
         "25.6",
         "39.0",
         "977.1",
         "1",
         "6",
         "6"
        ],
        [
         "4",
         "BETTY",
         "2023",
         "2",
         "BATANES",
         "SABTANG",
         "575",
         "1955",
         "6",
         "0",
         "0",
         "0",
         "0",
         "0",
         "0",
         "575",
         "296521.75",
         "BASCO",
         "BATANES",
         "19.89123103",
         "5/27/2023",
         "6/1/2023",
         "133.0",
         "195",
         "STY",
         "25.6",
         "39.0",
         "977.1",
         "1",
         "6",
         "6"
        ]
       ],
       "shape": {
        "columns": 30,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Typhoon Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Region</th>\n",
       "      <th>Province</th>\n",
       "      <th>City/Municipality</th>\n",
       "      <th>Families</th>\n",
       "      <th>Person</th>\n",
       "      <th>Brgy</th>\n",
       "      <th>Dead</th>\n",
       "      <th>Injured/Ill</th>\n",
       "      <th>...</th>\n",
       "      <th>PAR_END</th>\n",
       "      <th>Duration_in_PAR_Hours</th>\n",
       "      <th>Max_Sustained_Wind_kph</th>\n",
       "      <th>Typhoon_Type</th>\n",
       "      <th>Max_24hr_Rainfall_mm</th>\n",
       "      <th>Total_Storm_Rainfall_mm</th>\n",
       "      <th>Min_Pressure_hPa</th>\n",
       "      <th>Weather_Station_Mapped</th>\n",
       "      <th>Weather_Records_Found</th>\n",
       "      <th>Weather_Days_Covered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BETTY</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>BATANES</td>\n",
       "      <td>BASCO</td>\n",
       "      <td>3608</td>\n",
       "      <td>11120</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6/1/2023</td>\n",
       "      <td>133.0</td>\n",
       "      <td>195</td>\n",
       "      <td>STY</td>\n",
       "      <td>25.6</td>\n",
       "      <td>39.00</td>\n",
       "      <td>977.1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BETTY</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>BATANES</td>\n",
       "      <td>ITBAYAT</td>\n",
       "      <td>968</td>\n",
       "      <td>3028</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6/1/2023</td>\n",
       "      <td>133.0</td>\n",
       "      <td>195</td>\n",
       "      <td>STY</td>\n",
       "      <td>20.3</td>\n",
       "      <td>32.71</td>\n",
       "      <td>1000.5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BETTY</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>BATANES</td>\n",
       "      <td>IVANA</td>\n",
       "      <td>444</td>\n",
       "      <td>1532</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6/1/2023</td>\n",
       "      <td>133.0</td>\n",
       "      <td>195</td>\n",
       "      <td>STY</td>\n",
       "      <td>25.6</td>\n",
       "      <td>39.00</td>\n",
       "      <td>977.1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BETTY</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>BATANES</td>\n",
       "      <td>MAHATAO</td>\n",
       "      <td>575</td>\n",
       "      <td>1792</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6/1/2023</td>\n",
       "      <td>133.0</td>\n",
       "      <td>195</td>\n",
       "      <td>STY</td>\n",
       "      <td>25.6</td>\n",
       "      <td>39.00</td>\n",
       "      <td>977.1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>BETTY</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>BATANES</td>\n",
       "      <td>SABTANG</td>\n",
       "      <td>575</td>\n",
       "      <td>1955</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6/1/2023</td>\n",
       "      <td>133.0</td>\n",
       "      <td>195</td>\n",
       "      <td>STY</td>\n",
       "      <td>25.6</td>\n",
       "      <td>39.00</td>\n",
       "      <td>977.1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Typhoon Name  Year  Region Province City/Municipality  Families  Person  \\\n",
       "0        BETTY  2023       2  BATANES             BASCO      3608   11120   \n",
       "1        BETTY  2023       2  BATANES           ITBAYAT       968    3028   \n",
       "2        BETTY  2023       2  BATANES             IVANA       444    1532   \n",
       "3        BETTY  2023       2  BATANES           MAHATAO       575    1792   \n",
       "4        BETTY  2023       2  BATANES           SABTANG       575    1955   \n",
       "\n",
       "   Brgy  Dead  Injured/Ill  ...   PAR_END  Duration_in_PAR_Hours  \\\n",
       "0     6     0            0  ...  6/1/2023                  133.0   \n",
       "1     5     0            0  ...  6/1/2023                  133.0   \n",
       "2     4     0            0  ...  6/1/2023                  133.0   \n",
       "3     4     0            0  ...  6/1/2023                  133.0   \n",
       "4     6     0            0  ...  6/1/2023                  133.0   \n",
       "\n",
       "   Max_Sustained_Wind_kph  Typhoon_Type  Max_24hr_Rainfall_mm  \\\n",
       "0                     195           STY                  25.6   \n",
       "1                     195           STY                  20.3   \n",
       "2                     195           STY                  25.6   \n",
       "3                     195           STY                  25.6   \n",
       "4                     195           STY                  25.6   \n",
       "\n",
       "   Total_Storm_Rainfall_mm Min_Pressure_hPa Weather_Station_Mapped  \\\n",
       "0                    39.00            977.1                      1   \n",
       "1                    32.71           1000.5                      1   \n",
       "2                    39.00            977.1                      1   \n",
       "3                    39.00            977.1                      1   \n",
       "4                    39.00            977.1                      1   \n",
       "\n",
       "   Weather_Records_Found Weather_Days_Covered  \n",
       "0                      6                    6  \n",
       "1                      6                    6  \n",
       "2                      6                    6  \n",
       "3                      6                    6  \n",
       "4                      6                    6  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/typhoon_impact_with_extreme_weather.csv')\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bd6f4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1776 entries, 0 to 1775\n",
      "Data columns (total 30 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   Typhoon Name             1776 non-null   object \n",
      " 1   Year                     1776 non-null   int64  \n",
      " 2   Region                   1776 non-null   int64  \n",
      " 3   Province                 1776 non-null   object \n",
      " 4   City/Municipality        1776 non-null   object \n",
      " 5   Families                 1776 non-null   int64  \n",
      " 6   Person                   1776 non-null   int64  \n",
      " 7   Brgy                     1776 non-null   int64  \n",
      " 8   Dead                     1776 non-null   int64  \n",
      " 9   Injured/Ill              1776 non-null   int64  \n",
      " 10  Missing                  1776 non-null   int64  \n",
      " 11  Totally                  1776 non-null   int64  \n",
      " 12  Partially                1776 non-null   int64  \n",
      " 13  Total                    1776 non-null   int64  \n",
      " 14  Quantity                 1776 non-null   int64  \n",
      " 15  Cost                     1776 non-null   float64\n",
      " 16  Nearest_Station          1776 non-null   object \n",
      " 17  Station_Province         1776 non-null   object \n",
      " 18  Distance_km              1776 non-null   float64\n",
      " 19  PAR_START                1776 non-null   object \n",
      " 20  PAR_END                  1776 non-null   object \n",
      " 21  Duration_in_PAR_Hours    1776 non-null   float64\n",
      " 22  Max_Sustained_Wind_kph   1776 non-null   int64  \n",
      " 23  Typhoon_Type             1776 non-null   object \n",
      " 24  Max_24hr_Rainfall_mm     1776 non-null   float64\n",
      " 25  Total_Storm_Rainfall_mm  1776 non-null   float64\n",
      " 26  Min_Pressure_hPa         1776 non-null   float64\n",
      " 27  Weather_Station_Mapped   1776 non-null   int64  \n",
      " 28  Weather_Records_Found    1776 non-null   int64  \n",
      " 29  Weather_Days_Covered     1776 non-null   int64  \n",
      "dtypes: float64(6), int64(16), object(8)\n",
      "memory usage: 416.4+ KB\n"
     ]
    }
   ],
   "source": [
    "# Display dataset info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0f9327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "0",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "287ed893-000d-4be6-b430-f94857826d9b",
       "rows": [
        [
         "Typhoon Name",
         "0"
        ],
        [
         "Year",
         "0"
        ],
        [
         "Region",
         "0"
        ],
        [
         "Province",
         "0"
        ],
        [
         "City/Municipality",
         "0"
        ],
        [
         "Families",
         "0"
        ],
        [
         "Person",
         "0"
        ],
        [
         "Brgy",
         "0"
        ],
        [
         "Dead",
         "0"
        ],
        [
         "Injured/Ill",
         "0"
        ],
        [
         "Missing",
         "0"
        ],
        [
         "Totally",
         "0"
        ],
        [
         "Partially",
         "0"
        ],
        [
         "Total",
         "0"
        ],
        [
         "Quantity",
         "0"
        ],
        [
         "Cost",
         "0"
        ],
        [
         "Nearest_Station",
         "0"
        ],
        [
         "Station_Province",
         "0"
        ],
        [
         "Distance_km",
         "0"
        ],
        [
         "PAR_START",
         "0"
        ],
        [
         "PAR_END",
         "0"
        ],
        [
         "Duration_in_PAR_Hours",
         "0"
        ],
        [
         "Max_Sustained_Wind_kph",
         "0"
        ],
        [
         "Typhoon_Type",
         "0"
        ],
        [
         "Max_24hr_Rainfall_mm",
         "0"
        ],
        [
         "Total_Storm_Rainfall_mm",
         "0"
        ],
        [
         "Min_Pressure_hPa",
         "0"
        ],
        [
         "Weather_Station_Mapped",
         "0"
        ],
        [
         "Weather_Records_Found",
         "0"
        ],
        [
         "Weather_Days_Covered",
         "0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 30
       }
      },
      "text/plain": [
       "Typhoon Name               0\n",
       "Year                       0\n",
       "Region                     0\n",
       "Province                   0\n",
       "City/Municipality          0\n",
       "Families                   0\n",
       "Person                     0\n",
       "Brgy                       0\n",
       "Dead                       0\n",
       "Injured/Ill                0\n",
       "Missing                    0\n",
       "Totally                    0\n",
       "Partially                  0\n",
       "Total                      0\n",
       "Quantity                   0\n",
       "Cost                       0\n",
       "Nearest_Station            0\n",
       "Station_Province           0\n",
       "Distance_km                0\n",
       "PAR_START                  0\n",
       "PAR_END                    0\n",
       "Duration_in_PAR_Hours      0\n",
       "Max_Sustained_Wind_kph     0\n",
       "Typhoon_Type               0\n",
       "Max_24hr_Rainfall_mm       0\n",
       "Total_Storm_Rainfall_mm    0\n",
       "Min_Pressure_hPa           0\n",
       "Weather_Station_Mapped     0\n",
       "Weather_Records_Found      0\n",
       "Weather_Days_Covered       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb285a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Typhoon Name', 'Year', 'Region', 'Province', 'City/Municipality',\n",
       "       'Families', 'Person', 'Brgy', 'Dead', 'Injured/Ill', 'Missing',\n",
       "       'Totally', 'Partially', 'Total', 'Quantity', 'Cost', 'Nearest_Station',\n",
       "       'Station_Province', 'Distance_km', 'PAR_START', 'PAR_END',\n",
       "       'Duration_in_PAR_Hours', 'Max_Sustained_Wind_kph', 'Typhoon_Type',\n",
       "       'Max_24hr_Rainfall_mm', 'Total_Storm_Rainfall_mm', 'Min_Pressure_hPa',\n",
       "       'Weather_Station_Mapped', 'Weather_Records_Found',\n",
       "       'Weather_Days_Covered'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display all columns\n",
    "print(\"All columns in dataset:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9aa6268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert column names to lowercase for consistency\n",
    "df.columns = df.columns.str.lower().str.replace(' ', '_').str.replace('/', '_')\n",
    "print(\"Columns after normalization:\")\n",
    "print(df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e1780",
   "metadata": {},
   "source": [
    "## 2. Feature and Target Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f3557d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input features and output targets\n",
    "INPUT_FEATURES = [\n",
    "    'max_sustained_wind_kph',\n",
    "    'typhoon_type',\n",
    "    'max_24hr_rainfall_mm',\n",
    "    'total_storm_rainfall_mm',\n",
    "    'min_pressure_hpa'\n",
    "]\n",
    "\n",
    "OUTPUT_TARGETS = [\n",
    "    'families',\n",
    "    'person',\n",
    "    'brgy',\n",
    "    'dead',\n",
    "    'injured_ill',\n",
    "    'missing',\n",
    "    'cost',\n",
    "    'partially',\n",
    "    'totally'\n",
    "]\n",
    "\n",
    "print(\"Input Features:\", INPUT_FEATURES)\n",
    "print(\"\\nOutput Targets:\", OUTPUT_TARGETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8c0bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if all required columns exist\n",
    "print(\"Checking for required columns...\")\n",
    "missing_inputs = [col for col in INPUT_FEATURES if col not in df.columns]\n",
    "missing_outputs = [col for col in OUTPUT_TARGETS if col not in df.columns]\n",
    "\n",
    "if missing_inputs:\n",
    "    print(f\"Missing input columns: {missing_inputs}\")\n",
    "if missing_outputs:\n",
    "    print(f\"Missing output columns: {missing_outputs}\")\n",
    "    \n",
    "if not missing_inputs and not missing_outputs:\n",
    "    print(\"All required columns are present!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2311871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics for input features\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"INPUT FEATURES STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "df[INPUT_FEATURES].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1e3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display statistics for output targets\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"OUTPUT TARGETS STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "df[OUTPUT_TARGETS].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426326d7",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8ebeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check unique values of typhoon_type\n",
    "print(\"Unique typhoon types:\")\n",
    "print(df['typhoon_type'].unique())\n",
    "print(f\"\\nNumber of unique types: {df['typhoon_type'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02689385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode typhoon_type - using ordinal encoding based on intensity\n",
    "# STY (Super Typhoon) > TY (Typhoon) > STS (Severe Tropical Storm) > TS (Tropical Storm) > TD (Tropical Depression)\n",
    "typhoon_type_mapping = {\n",
    "    'TD': 0,   # Tropical Depression\n",
    "    'TS': 1,   # Tropical Storm\n",
    "    'STS': 2,  # Severe Tropical Storm\n",
    "    'TY': 3,   # Typhoon\n",
    "    'STY': 4   # Super Typhoon\n",
    "}\n",
    "\n",
    "# Create a copy for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Apply mapping\n",
    "df_processed['typhoon_type_encoded'] = df_processed['typhoon_type'].map(typhoon_type_mapping)\n",
    "\n",
    "print(\"Typhoon type encoding:\")\n",
    "for k, v in typhoon_type_mapping.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "# Update input features to use encoded version\n",
    "INPUT_FEATURES_ENCODED = [\n",
    "    'max_sustained_wind_kph',\n",
    "    'typhoon_type_encoded',\n",
    "    'max_24hr_rainfall_mm',\n",
    "    'total_storm_rainfall_mm',\n",
    "    'min_pressure_hpa'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6182a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare feature matrix (X) and target matrix (y)\n",
    "X = df_processed[INPUT_FEATURES_ENCODED].copy()\n",
    "y = df_processed[OUTPUT_TARGETS].copy()\n",
    "\n",
    "# Handle any missing values\n",
    "X = X.fillna(X.median())\n",
    "y = y.fillna(0)\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target matrix shape: {y.shape}\")\n",
    "print(f\"\\nFeature columns: {X.columns.tolist()}\")\n",
    "print(f\"Target columns: {y.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c965cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Testing set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef90cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features using StandardScaler\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "print(\"Feature scaling completed!\")\n",
    "print(\"\\nScaled training features statistics:\")\n",
    "print(X_train_scaled.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593e96cb",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fce681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize input feature distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(INPUT_FEATURES_ENCODED):\n",
    "    if i < len(axes):\n",
    "        axes[i].hist(X[col], bins=30, edgecolor='black', alpha=0.7)\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "# Hide extra subplot\n",
    "if len(INPUT_FEATURES_ENCODED) < len(axes):\n",
    "    axes[-1].set_visible(False)\n",
    "\n",
    "plt.suptitle('Input Features Distribution', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcd9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize output target distributions\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, col in enumerate(OUTPUT_TARGETS):\n",
    "    axes[i].hist(y[col], bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
    "    axes[i].set_title(f'Distribution of {col}')\n",
    "    axes[i].set_xlabel(col)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.suptitle('Output Targets Distribution', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffdd2c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap between inputs and outputs\n",
    "correlation_data = pd.concat([X, y], axis=1)\n",
    "correlation_matrix = correlation_data.corr()\n",
    "\n",
    "# Extract correlation between inputs and outputs\n",
    "input_output_corr = correlation_matrix.loc[INPUT_FEATURES_ENCODED, OUTPUT_TARGETS]\n",
    "\n",
    "plt.figure(figsize=(14, 8))\n",
    "sns.heatmap(input_output_corr, annot=True, fmt='.2f', cmap='RdBu_r', center=0,\n",
    "            linewidths=0.5, vmin=-1, vmax=1)\n",
    "plt.title('Correlation between Input Features and Output Targets', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Output Targets')\n",
    "plt.ylabel('Input Features')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d246a87",
   "metadata": {},
   "source": [
    "## 5. Multi-Output Regression with Optuna Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7044f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_pred, target_names):\n",
    "    \"\"\"\n",
    "    Evaluate multi-output regression model performance.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True target values\n",
    "    y_pred : array-like\n",
    "        Predicted target values\n",
    "    target_names : list\n",
    "        Names of target variables\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict: Dictionary containing evaluation metrics for each target\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for i, target in enumerate(target_names):\n",
    "        y_t = y_true[:, i] if isinstance(y_true, np.ndarray) else y_true.iloc[:, i]\n",
    "        y_p = y_pred[:, i]\n",
    "        \n",
    "        mse = mean_squared_error(y_t, y_p)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_t, y_p)\n",
    "        r2 = r2_score(y_t, y_p)\n",
    "        \n",
    "        results[target] = {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2\n",
    "        }\n",
    "    \n",
    "    # Calculate overall metrics\n",
    "    overall_mse = mean_squared_error(y_true, y_pred)\n",
    "    overall_r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    results['overall'] = {\n",
    "        'MSE': overall_mse,\n",
    "        'RMSE': np.sqrt(overall_mse),\n",
    "        'R2': overall_r2\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def print_evaluation_results(results, title=\"Model Evaluation Results\"):\n",
    "    \"\"\"\n",
    "    Print formatted evaluation results.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(title)\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\n{'Target':<20} {'MSE':>15} {'RMSE':>15} {'MAE':>15} {'R2':>10}\")\n",
    "    print(\"-\"*75)\n",
    "    \n",
    "    for target, metrics in results.items():\n",
    "        if target != 'overall':\n",
    "            mae = metrics.get('MAE', 'N/A')\n",
    "            mae_str = f\"{mae:>15.2f}\" if isinstance(mae, (int, float)) else f\"{mae:>15}\"\n",
    "            print(f\"{target:<20} {metrics['MSE']:>15.2f} {metrics['RMSE']:>15.2f} {mae_str} {metrics['R2']:>10.4f}\")\n",
    "    \n",
    "    print(\"-\"*75)\n",
    "    overall = results['overall']\n",
    "    print(f\"{'OVERALL':<20} {overall['MSE']:>15.2f} {overall['RMSE']:>15.2f} {'':>15} {overall['R2']:>10.4f}\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d5e4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optuna objective function for XGBoost Multi-Output Regressor\n",
    "def optuna_xgb_objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for XGBoost hyperparameter optimization.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 1),\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Create Multi-Output XGBoost Regressor\n",
    "    base_model = xgb.XGBRegressor(**params)\n",
    "    model = MultiOutputRegressor(base_model)\n",
    "    \n",
    "    # Use cross-validation\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = []\n",
    "    for train_idx, val_idx in kfold.split(X_train_scaled):\n",
    "        X_cv_train = X_train_scaled.iloc[train_idx]\n",
    "        X_cv_val = X_train_scaled.iloc[val_idx]\n",
    "        y_cv_train = y_train.iloc[train_idx]\n",
    "        y_cv_val = y_train.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_cv_train, y_cv_train)\n",
    "        y_pred = model.predict(X_cv_val)\n",
    "        \n",
    "        r2 = r2_score(y_cv_val, y_pred)\n",
    "        cv_scores.append(r2)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "\n",
    "print(\"Optuna objective function defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8288a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna optimization for XGBoost\n",
    "print(\"=\"*80)\n",
    "print(\"OPTUNA HYPERPARAMETER OPTIMIZATION - XGBOOST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create Optuna study\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "sampler = TPESampler(seed=42)\n",
    "xgb_study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    study_name='xgb_multioutput_optimization',\n",
    "    sampler=sampler\n",
    ")\n",
    "\n",
    "# Optimize\n",
    "print(\"\\nOptimizing XGBoost hyperparameters...\")\n",
    "xgb_study.optimize(optuna_xgb_objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Display best parameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBOOST OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best R2 Score (CV): {xgb_study.best_value:.4f}\")\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in xgb_study.best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2db63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optuna objective function for Random Forest Multi-Output Regressor\n",
    "def optuna_rf_objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna objective function for Random Forest hyperparameter optimization.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]),\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "    \n",
    "    # Create Multi-Output Random Forest Regressor\n",
    "    model = RandomForestRegressor(**params)\n",
    "    \n",
    "    # Use cross-validation\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    cv_scores = []\n",
    "    for train_idx, val_idx in kfold.split(X_train_scaled):\n",
    "        X_cv_train = X_train_scaled.iloc[train_idx]\n",
    "        X_cv_val = X_train_scaled.iloc[val_idx]\n",
    "        y_cv_train = y_train.iloc[train_idx]\n",
    "        y_cv_val = y_train.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_cv_train, y_cv_train)\n",
    "        y_pred = model.predict(X_cv_val)\n",
    "        \n",
    "        r2 = r2_score(y_cv_val, y_pred)\n",
    "        cv_scores.append(r2)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "\n",
    "print(\"Random Forest objective function defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56466a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Optuna optimization for Random Forest\n",
    "print(\"=\"*80)\n",
    "print(\"OPTUNA HYPERPARAMETER OPTIMIZATION - RANDOM FOREST\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create Optuna study\n",
    "rf_study = optuna.create_study(\n",
    "    direction='maximize',\n",
    "    study_name='rf_multioutput_optimization',\n",
    "    sampler=sampler\n",
    ")\n",
    "\n",
    "# Optimize\n",
    "print(\"\\nOptimizing Random Forest hyperparameters...\")\n",
    "rf_study.optimize(optuna_rf_objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "# Display best parameters\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RANDOM FOREST OPTIMIZATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best R2 Score (CV): {rf_study.best_value:.4f}\")\n",
    "print(\"\\nBest Hyperparameters:\")\n",
    "for param, value in rf_study.best_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dce212a",
   "metadata": {},
   "source": [
    "## 6. Train Final Models with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55d18f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final XGBoost model with best parameters\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING FINAL XGBOOST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_xgb_params = xgb_study.best_params.copy()\n",
    "best_xgb_params['random_state'] = 42\n",
    "best_xgb_params['n_jobs'] = -1\n",
    "\n",
    "# Create and train the model\n",
    "xgb_base = xgb.XGBRegressor(**best_xgb_params)\n",
    "xgb_model = MultiOutputRegressor(xgb_base)\n",
    "xgb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_xgb = xgb_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "xgb_results = evaluate_model(y_test.values, y_pred_xgb, OUTPUT_TARGETS)\n",
    "print_evaluation_results(xgb_results, \"XGBOOST MULTI-OUTPUT REGRESSION RESULTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbe8287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train final Random Forest model with best parameters\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING FINAL RANDOM FOREST MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "best_rf_params = rf_study.best_params.copy()\n",
    "best_rf_params['random_state'] = 42\n",
    "best_rf_params['n_jobs'] = -1\n",
    "\n",
    "# Create and train the model\n",
    "rf_model = RandomForestRegressor(**best_rf_params)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_rf = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate\n",
    "rf_results = evaluate_model(y_test.values, y_pred_rf, OUTPUT_TARGETS)\n",
    "print_evaluation_results(rf_results, \"RANDOM FOREST MULTI-OUTPUT REGRESSION RESULTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adf7366",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0bd0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performances\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "comparison_data = []\n",
    "for target in OUTPUT_TARGETS:\n",
    "    comparison_data.append({\n",
    "        'Target': target,\n",
    "        'XGBoost_R2': xgb_results[target]['R2'],\n",
    "        'XGBoost_RMSE': xgb_results[target]['RMSE'],\n",
    "        'RF_R2': rf_results[target]['R2'],\n",
    "        'RF_RMSE': rf_results[target]['RMSE'],\n",
    "        'Best_Model': 'XGBoost' if xgb_results[target]['R2'] > rf_results[target]['R2'] else 'Random Forest'\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\")\n",
    "display(comparison_df)\n",
    "\n",
    "print(f\"\\nOverall Best Model: {'XGBoost' if xgb_results['overall']['R2'] > rf_results['overall']['R2'] else 'Random Forest'}\")\n",
    "print(f\"  XGBoost Overall R2: {xgb_results['overall']['R2']:.4f}\")\n",
    "print(f\"  Random Forest Overall R2: {rf_results['overall']['R2']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04407e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize R2 comparison\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "x = np.arange(len(OUTPUT_TARGETS))\n",
    "width = 0.35\n",
    "\n",
    "xgb_r2 = [xgb_results[t]['R2'] for t in OUTPUT_TARGETS]\n",
    "rf_r2 = [rf_results[t]['R2'] for t in OUTPUT_TARGETS]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, xgb_r2, width, label='XGBoost', color='steelblue', edgecolor='white')\n",
    "bars2 = ax.bar(x + width/2, rf_r2, width, label='Random Forest', color='coral', edgecolor='white')\n",
    "\n",
    "ax.set_xlabel('Target Variable', fontsize=12)\n",
    "ax.set_ylabel('R² Score', fontsize=12)\n",
    "ax.set_title('Model Comparison: R² Score by Target Variable', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(OUTPUT_TARGETS, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcac3c1",
   "metadata": {},
   "source": [
    "## 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c40cd98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance from Random Forest (it natively supports multi-output)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': INPUT_FEATURES_ENCODED,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FEATURE IMPORTANCE (Random Forest)\")\n",
    "print(\"=\"*60)\n",
    "print(feature_importance.to_string(index=False))\n",
    "\n",
    "# Plot feature importance\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(feature_importance)))\n",
    "bars = ax.barh(feature_importance['Feature'], feature_importance['Importance'], color=colors)\n",
    "\n",
    "ax.set_xlabel('Importance Score', fontsize=12)\n",
    "ax.set_ylabel('Feature', fontsize=12)\n",
    "ax.set_title('Feature Importance for Typhoon Impact Prediction', fontsize=14, fontweight='bold')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "# Add value labels\n",
    "for bar, importance in zip(bars, feature_importance['Importance']):\n",
    "    ax.text(bar.get_width() + 0.01, bar.get_y() + bar.get_height()/2,\n",
    "            f'{importance:.4f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6261452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature importance per target from XGBoost\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE IMPORTANCE PER TARGET (XGBoost)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "xgb_importance_per_target = {}\n",
    "for i, target in enumerate(OUTPUT_TARGETS):\n",
    "    estimator = xgb_model.estimators_[i]\n",
    "    importance = estimator.feature_importances_\n",
    "    xgb_importance_per_target[target] = importance\n",
    "\n",
    "importance_df = pd.DataFrame(xgb_importance_per_target, index=INPUT_FEATURES_ENCODED)\n",
    "print(importance_df.round(4))\n",
    "\n",
    "# Heatmap of feature importance per target\n",
    "plt.figure(figsize=(14, 6))\n",
    "sns.heatmap(importance_df, annot=True, fmt='.3f', cmap='YlOrRd', linewidths=0.5)\n",
    "plt.title('Feature Importance per Target Variable (XGBoost)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Target Variable')\n",
    "plt.ylabel('Input Feature')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25932a4",
   "metadata": {},
   "source": [
    "## 9. Prediction vs Actual Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c102004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on overall R2\n",
    "best_model_name = 'XGBoost' if xgb_results['overall']['R2'] > rf_results['overall']['R2'] else 'Random Forest'\n",
    "best_predictions = y_pred_xgb if best_model_name == 'XGBoost' else y_pred_rf\n",
    "best_results = xgb_results if best_model_name == 'XGBoost' else rf_results\n",
    "\n",
    "print(f\"Using {best_model_name} for visualization (Best Overall R2: {best_results['overall']['R2']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05f770e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Predicted vs Actual for each target\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target in enumerate(OUTPUT_TARGETS):\n",
    "    ax = axes[i]\n",
    "    actual = y_test.iloc[:, i].values\n",
    "    predicted = best_predictions[:, i]\n",
    "    \n",
    "    ax.scatter(actual, predicted, alpha=0.5, s=20)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(actual.min(), predicted.min())\n",
    "    max_val = max(actual.max(), predicted.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', linewidth=2, label='Perfect Prediction')\n",
    "    \n",
    "    ax.set_xlabel('Actual', fontsize=10)\n",
    "    ax.set_ylabel('Predicted', fontsize=10)\n",
    "    ax.set_title(f'{target}\\nR² = {best_results[target][\"R2\"]:.4f}', fontsize=11)\n",
    "    ax.legend(fontsize=8)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Predicted vs Actual Values ({best_model_name})', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0305e1",
   "metadata": {},
   "source": [
    "## 10. Residual Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8565c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual plots\n",
    "fig, axes = plt.subplots(3, 3, figsize=(15, 15))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, target in enumerate(OUTPUT_TARGETS):\n",
    "    ax = axes[i]\n",
    "    actual = y_test.iloc[:, i].values\n",
    "    predicted = best_predictions[:, i]\n",
    "    residuals = actual - predicted\n",
    "    \n",
    "    ax.scatter(predicted, residuals, alpha=0.5, s=20)\n",
    "    ax.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('Predicted', fontsize=10)\n",
    "    ax.set_ylabel('Residuals', fontsize=10)\n",
    "    ax.set_title(f'{target}', fontsize=11)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Residual Plots ({best_model_name})', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e30fe7",
   "metadata": {},
   "source": [
    "## 11. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ec153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create timestamp for file naming\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n",
    "# Save models\n",
    "joblib.dump(xgb_model, f'../models/xgb_multioutput_model_{timestamp}.joblib')\n",
    "joblib.dump(rf_model, f'../models/rf_multioutput_model_{timestamp}.joblib')\n",
    "joblib.dump(scaler_X, f'../models/feature_scaler_{timestamp}.joblib')\n",
    "\n",
    "print(f\"Models saved with timestamp: {timestamp}\")\n",
    "print(f\"  - XGBoost model: xgb_multioutput_model_{timestamp}.joblib\")\n",
    "print(f\"  - Random Forest model: rf_multioutput_model_{timestamp}.joblib\")\n",
    "print(f\"  - Feature scaler: feature_scaler_{timestamp}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7690633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_summary = []\n",
    "for target in OUTPUT_TARGETS:\n",
    "    results_summary.append({\n",
    "        'Target': target,\n",
    "        'XGBoost_R2': xgb_results[target]['R2'],\n",
    "        'XGBoost_RMSE': xgb_results[target]['RMSE'],\n",
    "        'XGBoost_MAE': xgb_results[target]['MAE'],\n",
    "        'RF_R2': rf_results[target]['R2'],\n",
    "        'RF_RMSE': rf_results[target]['RMSE'],\n",
    "        'RF_MAE': rf_results[target]['MAE']\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results_summary)\n",
    "results_df.to_csv(f'../models/mor_results_{timestamp}.csv', index=False)\n",
    "print(f\"Results saved to: mor_results_{timestamp}.csv\")\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37bac390",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best hyperparameters\n",
    "hyperparams = {\n",
    "    'xgboost': xgb_study.best_params,\n",
    "    'random_forest': rf_study.best_params\n",
    "}\n",
    "\n",
    "joblib.dump(hyperparams, f'../models/best_hyperparameters_{timestamp}.joblib')\n",
    "print(f\"Best hyperparameters saved to: best_hyperparameters_{timestamp}.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b0a106",
   "metadata": {},
   "source": [
    "## 12. Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8c2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sample predictions\n",
    "print(\"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select a few test samples\n",
    "sample_indices = np.random.choice(len(X_test), size=5, replace=False)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    print(f\"\\n--- Sample {idx} ---\")\n",
    "    print(\"Input Features:\")\n",
    "    for i, col in enumerate(INPUT_FEATURES_ENCODED):\n",
    "        print(f\"  {col}: {X_test.iloc[idx, i]:.2f}\")\n",
    "    \n",
    "    print(\"\\nActual vs Predicted:\")\n",
    "    print(f\"{'Target':<15} {'Actual':>12} {'Predicted':>12} {'Error':>12}\")\n",
    "    print(\"-\"*55)\n",
    "    for i, target in enumerate(OUTPUT_TARGETS):\n",
    "        actual = y_test.iloc[idx, i]\n",
    "        predicted = best_predictions[sample_indices.tolist().index(idx), i]\n",
    "        error = actual - predicted\n",
    "        print(f\"{target:<15} {actual:>12.2f} {predicted:>12.2f} {error:>12.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "716662d4",
   "metadata": {},
   "source": [
    "## 13. Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bce057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAACLI report exported to: ../cluster_explanations_MAACLI_KMeans.txt\n"
     ]
    }
   ],
   "source": [
    "# Print final summary\n",
    "print(\"=\"*80)\n",
    "print(\"MULTI-OUTPUT REGRESSION - FINAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. DATASET INFORMATION\")\n",
    "print(f\"   - Total samples: {len(df)}\")\n",
    "print(f\"   - Training samples: {len(X_train)}\")\n",
    "print(f\"   - Testing samples: {len(X_test)}\")\n",
    "print(f\"   - Input features: {len(INPUT_FEATURES_ENCODED)}\")\n",
    "print(f\"   - Output targets: {len(OUTPUT_TARGETS)}\")\n",
    "\n",
    "print(\"\\n2. INPUT FEATURES\")\n",
    "for feat in INPUT_FEATURES_ENCODED:\n",
    "    print(f\"   - {feat}\")\n",
    "\n",
    "print(\"\\n3. OUTPUT TARGETS\")\n",
    "for target in OUTPUT_TARGETS:\n",
    "    print(f\"   - {target}\")\n",
    "\n",
    "print(\"\\n4. MODEL PERFORMANCE (Test Set)\")\n",
    "print(f\"   XGBoost:\")\n",
    "print(f\"     - Overall R²: {xgb_results['overall']['R2']:.4f}\")\n",
    "print(f\"     - Overall RMSE: {xgb_results['overall']['RMSE']:.2f}\")\n",
    "print(f\"   Random Forest:\")\n",
    "print(f\"     - Overall R²: {rf_results['overall']['R2']:.4f}\")\n",
    "print(f\"     - Overall RMSE: {rf_results['overall']['RMSE']:.2f}\")\n",
    "\n",
    "print(f\"\\n5. BEST MODEL: {best_model_name}\")\n",
    "print(f\"   - Overall R²: {best_results['overall']['R2']:.4f}\")\n",
    "\n",
    "print(\"\\n6. TOP CONTRIBUTING FEATURES (Random Forest)\")\n",
    "for _, row in feature_importance.head(3).iterrows():\n",
    "    print(f\"   - {row['Feature']}: {row['Importance']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"END OF MULTI-OUTPUT REGRESSION ANALYSIS\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
